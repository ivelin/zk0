[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.metadata]
allow-direct-references = true

[project]
name = "zk0"
version = "0.1.13"
description = "Federated Learning with SmolVLA and Flower (SO-100 Robotics Example)"
license = "Apache-2.0"
authors = [
    { name = "Ivelin Ivanov", email = "ivelin@zk0.bot" },
    { name = "The Flower Authors", email = "hello@flower.ai" },
]

dependencies = [
    "flwr[simulation]==1.20.0",
    "transformers>=4.50.3,<4.52.0",
    "lerobot[smolvla]>=0.3.0",
    "flwr-datasets>=0.5.0",
    "torch>=2.5.0",
    "evaluate>=0.4.0,<1.0",
    "scikit-learn>=1.3.1, <2.0",
    "num2words>=0.5.14",
    "accelerate>=1.7.0",
]

[project.optional-dependencies]
test = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.12.0",
    "pytest-asyncio>=0.23.0",
    "pytest-xdist>=3.6.0",
]

[tool.hatch.build.targets.wheel]
packages = ["src"]

[tool.flwr.app]
publisher = "flwrlabs"

[tool.flwr.app.components]
serverapp = "src.server_app:app"
clientapp = "src.client_app:app"

[tool.flwr.app.config]
num-server-rounds = 20 # train rounds
local-epochs = 1000 # steps/epochs per round (reduced from 100 to prevent hanging)
model-name = "lerobot/smolvla_base"
fraction-fit = 0.5
fraction-evaluate = 1 # Evaluate all clients
batch_size = 64 # Batch size for training and evaluation (match standalone train script)
server-device = "gpu" # could be also "cpu"
use-wandb = true # requires WANDB_API_KEY env var set
eval-frequency = 2 # Evaluate every eval-frequency rounds
eval_mode = "quick" # Quick evaluation vs full (full is very slow)

[tool.zk0.logging]
# Application logging configuration (automatically applied to Flower/Ray)
level = "DEBUG"  # DEBUG, INFO, WARNING, ERROR, CRITICAL - Set to DEBUG for detailed troubleshooting
enable_grpc_logging = false  # Disable gRPC network communication logs to reduce noise
enable_ray_logging = true   # Enable Ray distributed execution logs
enable_audit_logging = false  # Disable JSON audit logging (requires Flower Enterprise)
log_format = "detailed"  # simple, detailed, json
file_rotation = "500 MB"  # File rotation size
file_retention = "10 days"  # Log retention period
ray_log_to_driver = true  # Route Ray worker logs to driver (server)
ray_color_prefix = true  # Enable colored Ray actor prefixes
# Note: Ray logging configuration (ray_dedup_logs, ray_log_to_stderr) is handled via environment variables in train.sh
# for simulation mode, as Ray must be configured before ray.init() is called by Flower

[tool.flwr.federations]
default = "local-simulation-serialized-gpu" # Serialized GPU execution prevents SafeTensors issues

[tool.flwr.federations.local-simulation]
options.num-supernodes = 4 # 4 clients for 4 SO-100 tasks (consistent with GPU version)

[tool.flwr.federations.local-simulation-gpu]
options.num-supernodes = 4 # 4 clients for 4 SO-100 tasks
options.backend.client-resources.num-cpus = 4 # each ClientApp assumes to use 4CPUs
options.backend.client-resources.num-gpus = 0.25 # SmolVLA is more memory efficient, allowing more parallelism

[tool.flwr.federations.local-simulation-serialized-gpu]
options.num-supernodes = 4 # 4 clients for test
options.backend.client-resources.num-cpus = 4 # dedicated CPU resources for serialized execution
options.backend.client-resources.num-gpus = 1.0 # full GPU access, but serialized to prevent conflicts
backend.client-resources.memory = "16GB"  # Per client actor; serialized mode uses one at a time
options.backend.max-parallelism = 1 # serialize execution - only 1 client runs at a time

[tool.flwr.federations.local-simulation-multiprocess]
options.num-supernodes = 4 # 4 clients for 4 SO-100 tasks
options.backend.type = "multiprocess" # Use Python multiprocessing instead of Ray
options.backend.client-resources.num-cpus = 4 # CPU resources per client
options.backend.client-resources.num-gpus = 0.25 # GPU resources per client (avoids conflicts)

[tool.zk0.datasets]
clients = [
    {name = "lerobot/svla_so100_pickplace", description = "Pick and place operations - VALIDATED CLEAN", client_id = 0, last_n_episodes_for_eval = 3},
    {name = "ethanCSL/direction_test", description = "turn to the right side - VALIDATED CLEAN", client_id = 1, last_n_episodes_for_eval = 3},
    {name = "gimarchetti/so101-winnie-us5", description = "rub the plush toy with the bottle - VALIDATED CLEAN", client_id = 2, last_n_episodes_for_eval = 3},
    {name = "sergiov2000/so100_movella_ball_usability_u3_stack1", description = "Stack the ball on top of the lego structure. - VALIDATED CLEAN", client_id = 3, last_n_episodes_for_eval = 3}
]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--verbose",
    "--tb=short",
    "--strict-markers",
    "--disable-warnings",
    "--durations=10",
    "-n=auto",
    "--cov=src",
    "--cov-report=term-missing",
    "--cov-report=html:htmlcov",
    "--cov-fail-under=80"
]
markers = [
    "unit: Unit tests",
    "integration: Integration tests",
    "slow: Slow running tests",
    "gpu: Tests requiring GPU"
]
