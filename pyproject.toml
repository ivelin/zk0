 [build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.metadata]
allow-direct-references = true

[project]
name = "zk0"
version = "0.2.9"
description = "Federated Learning with SmolVLA and Flower (SO-100 Robotics Example)"
license = "Apache-2.0"
authors = [
    { name = "Ivelin Ivanov", email = "ivelin@zk0.bot" },
    { name = "The Flower Authors", email = "hello@flower.ai" },
]

dependencies = [
    "flwr[simulation]==1.22.0",
    "transformers>=4.50.3,<4.52.0",
    # "lerobot[smolvla]>=0.3.3",  # Installed separately in train.sh to avoid torch version conflicts
    # "flwr-datasets>=0.5.0",
    # "torch>=2.9.0",  # torch version is managed separately to ensure compatibility with lerobot on DGX Spark and other CUDA environments
    "torchvision>=0.20.0",  # Compatible with torch 2.9.0+ to fix torchvision operator registration issues
    "evaluate>=0.4.0,<1.0",
    "scikit-learn>=1.3.1, <2.0",
    "num2words>=0.5.14",
    "accelerate>=1.7.0",
    "loguru>=0.7.3",
    "python-dotenv>=1.0.0,<2.0.0",
]

[project.optional-dependencies]
test = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.12.0",
    "pytest-asyncio>=0.23.0",
    "pytest-xdist>=3.6.0",
]

[tool.hatch.build.targets.wheel]
packages = ["src"]

[tool.flwr.app]
publisher = "flwrlabs"

[tool.flwr.app.components]
serverapp = "src.server_app:app"
clientapp = "src.client_app:app"

[tool.flwr.app.config]
num-server-rounds = 50 # Total should be 400 rounds for a combined 400x50 epochs = 20,000 epochs/steps per dataset
local-epochs = 20 # steps/epochs per round
model-name = "lerobot/smolvla_base" # fl fine tuned: "ivelin/zk0-smolvla-fl" # Baseline: "lerobot/smolvla_base"
fraction-fit = 1 # set to 1 for all clients to train each round
fraction-evaluate = 1 # Evaluate all clients
batch_size =  64 # Batch size for training and evaluation (match standalone train script)
server-device = "gpu" # could be also "cpu"
use-wandb = true # requires WANDB_API_KEY env var set
eval-frequency = 1 # Evaluate every eval-frequency rounds
eval_batches = 8 # Number of batches to evaluate (0 = all eval split batches (about 10 per episode), 8 = tiny/quick eval mode (about 1 episode))
checkpoint_interval = 10 # Save checkpoint every N rounds (0 = disabled)
hf_repo_id = "ivelin/zk0-smolvla-fl" # HF repo for final model push (optional; requires HF_TOKEN env var)
proximal_mu = 0.01 # Increased from 0.001 for better heterogeneity handling in SO-100
initial_lr = 1e-4 # Initial learning rate for FL training (reduced from 1e-3 for better FedProx convergence)
early_stopping_patience = 30 # Stop training if no eval loss improvement for this many rounds (0 = disabled)
dynamic_training_decay = true # Enable joint mu/LR adjustment based on eval trends for FL stability (default: disabled)
# New parameters for LR/MU scheduling enhancements (v0.2.6)
scheduler_type = "cosine_warm_restarts"  # "cosine" (current), "cosine_warm_restarts", "reduce_on_plateau"
cosine_warm_restarts_T_0 = 15  # Restart period (rounds) for warm restarts
cosine_warm_restarts_T_mult = 2  # Multiply T_0 after each restart (must be integer)
eta_min = 5e-7  # Minimum LR (global)
adaptive_lr_enabled = true  # Enable per-client LR boosts
adaptive_mu_enabled = true  # Enable dynamic mu
stall_patience = 5  # Rounds flat for ReduceLROnPlateau fallback
high_loss_multiplier = 2.0  # Threshold for hard clients (loss > avg * this)
lr_boost_factor = 1.15  # LR boost for hard clients
loss_std_threshold = 1.2  # Trigger mu increase if client loss std > this
mu_adjust_factor = 1.05  # Scale mu/LR on divergence
mu_min = 0.001  # Minimum mu value to prevent over-regularization
adjustment_window = 5  # Rounds for trend analysis (vs. 3)
spike_threshold = 0.5  # Hold if loss delta > this (prevent spikes)
max_adjust_factor = 1.05  # Cap adjustments early (<R20)
# outlier_client_ids = [1]  # Client IDs for priority boosts (e.g., direction_test) - dynamically detected

[tool.zk0.logging]
# Application logging configuration (automatically applied to Flower/Ray)
level = "DEBUG"  # DEBUG, INFO, WARNING, ERROR, CRITICAL - Set to DEBUG for detailed troubleshooting
enable_grpc_logging = false  # Disable gRPC network communication logs to reduce noise
enable_ray_logging = true   # Enable Ray distributed execution logs
enable_audit_logging = false  # Disable JSON audit logging (requires Flower Enterprise)
log_format = "detailed"  # simple, detailed, json
file_rotation = "500 MB"  # File rotation size
file_retention = "10 days"  # Log retention period
ray_log_to_driver = true  # Route Ray worker logs to driver (server)
ray_color_prefix = true  # Enable colored Ray actor prefixes
# Note: Ray logging configuration (ray_dedup_logs, ray_log_to_stderr) is handled via environment variables in train.sh
# for simulation mode, as Ray must be configured before ray.init() is called by Flower

[tool.flwr.federations]
default = "local-simulation-serialized-gpu" # Serialized GPU execution prevents SafeTensors issues

[tool.flwr.federations.local-simulation]
options.num-supernodes = 4 # 4 clients for 4 SO-100 tasks (consistent with GPU version)

[tool.flwr.federations.local-simulation-gpu]
options.num-supernodes = 4 # 4 clients for 4 SO-100 tasks
options.backend.client-resources.num-cpus = 4 # each ClientApp assumes to use 4CPUs
options.backend.client-resources.num-gpus = 0.2 # 20% for each for 4 clients and 20% for server process (eval). SmolVLA is more memory efficient, allowing more parallelism

[tool.flwr.federations.local-simulation-serialized-gpu]
options.num-supernodes = 4 # 4 clients for 4 SO-100 tasks
options.backend.client-resources.num-cpus = 4 # dedicated CPU resources for serialized execution
options.backend.client-resources.num-gpus = 1.0 # full GPU access, but serialized to prevent conflicts
backend.client-resources.memory = "16GB"  # Per client actor; serialized mode uses one at a time
options.backend.max-parallelism = 1 # serialize execution - only 1 client runs at a time

[tool.flwr.federations.local-simulation-multiprocess]
options.num-supernodes = 4 # 4 clients for 4 SO-100 tasks
options.backend.type = "multiprocess" # Use Python multiprocessing instead of Ray
options.backend.client-resources.num-cpus = 4 # CPU resources per client
options.backend.client-resources.num-gpus = 0.25 # GPU resources per client (avoids conflicts)

[tool.zk0.datasets]
clients = [
    {name = "shaunkirby/record-test", description = "Put the red LEGO in the bin", client_id = 0},
    {name = "ethanCSL/direction_test", description = "turn to the right side - VALIDATED CLEAN", client_id = 1},
    {name = "gimarchetti/so101-winnie-us5", description = "rub the plush toy with the bottle - VALIDATED CLEAN", client_id = 2},
    {name = "olingoudey/so101_stationary_mug445", description = "Put the stuffed animal in the mug. - VALIDATED CLEAN", client_id = 3}
]
server = [
    {name = "Hupy440/Two_Cubes_and_Two_Buckets_v2", description = "Pick up a cube. Is the cube red, put it in the white bucket. Is the cube white, put it in the red bucket. - VALIDATED CLEAN", first_n_episodes_for_eval = 3},
]

# Dataset loading configuration
force_opencv_backend = true  # Force OpenCV video backend to avoid TorchCodec frame corruption issues

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--verbose",
    "--tb=short",
    "--strict-markers",
    "--disable-warnings",
    "--durations=10",
    "-n=auto",
    "--cov=src",
    "--cov-report=term-missing",
    "--cov-report=html:htmlcov",
    "--cov-fail-under=80"
]
markers = [
    "unit: Unit tests",
    "integration: Integration tests",
    "slow: Slow running tests",
    "gpu: Tests requiring GPU"
]
