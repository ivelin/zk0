<h1 id="architecture">Architecture</h1>

<p>This document provides a detailed overview of the zk0 project’s architecture (v0.3.11), focusing on the federated learning system for training SmolVLA models on SO-100 robotics datasets. It adapts key concepts from the project’s implementation and incorporates advanced technical details, comparisons, reproducibility guidelines, evaluation mechanisms, and hyperparameter analysis.</p>

<table>
  <tbody>
    <tr>
      <td><a href="INSTALLATION.md">Installation Guide</a></td>
      <td><a href="NODE-OPERATORS.md">Node Operators</a></td>
      <td><a href="RUNNING.md">Running Simulations</a></td>
    </tr>
  </tbody>
</table>

<h2 id="overview">Overview</h2>

<p>The zk0 project implements a federated learning architecture using the Flower framework with SmolVLA models for robotics AI tasks. The system follows a client-server model where multiple clients train models locally on private SO-100 datasets, and a central server coordinates aggregation and evaluation. This ensures privacy-preserving distributed training while achieving performance comparable to centralized approaches.</p>

<p>Key goals:</p>
<ul>
  <li>Privacy: No raw data leaves client environments.</li>
  <li>Scalability: Supports large number of clients with heterogeneous data.</li>
  <li>Breadth: Enable contributions of edge cases from many diverse environments that no single contributor has access to alone.</li>
  <li>Efficiency: Optimized for GPU/CPU, with automatic device detection.</li>
</ul>

<p>The architecture is modular, drawing from Flower’s quickstart-lerobot example but adapted for SmolVLA and multi-repo datasets.</p>

<h2 id="directory-structure">Directory Structure</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>zk0/
├── .github/                     # GitHub workflows and configurations
├── .kilocode/                   # Kilo Code configuration and rules
├── docs/                        # Project documentation and images
│   └── images/                  # Images for documentation
├── src/                         # Source code modules
│   ├── configs/                 # Dataset and configuration files
│   └── server/                  # Server utilities and strategies
├── tests/                       # Unit and integration test suites
│   ├── integration/             # End-to-end federated learning tests
│   └── unit/                    # Individual component tests
└── outputs/                     # Generated outputs from runs
</code></pre></div></div>

<h2 id="core-components">Core Components</h2>

<h3 id="client-layer">Client Layer</h3>
<ul>
  <li><strong>SmolVLA Models</strong>: Vision-language-action models for robotics manipulation (450M parameters total).
    <ul>
      <li>Vision Encoder: SigLIP (frozen during training).</li>
      <li>Language Decoder: SmolLM2.</li>
      <li>Action Expert: Flow matching transformer (~100M trainable parameters).</li>
    </ul>
  </li>
  <li><strong>Local Datasets</strong>: SO-100 real-world robotics datasets, partitioned per client to ensure no overlap.</li>
  <li><strong>Training Logic</strong>: Local epochs with FedProx regularization (μ=0.01) for heterogeneous convergence.</li>
  <li><strong>Parameter Exchange</strong>: Secure transmission of model updates to the server.</li>
  <li><strong>Implementation</strong>: <a href="src/client_app.py"><code class="language-plaintext highlighter-rouge">src/client_app.py</code></a> extends Flower’s NumPyClient.</li>
</ul>

<h3 id="server-layer">Server Layer</h3>
<ul>
  <li><strong>Aggregation Engine</strong>: Flower’s FedProx strategy for parameter aggregation.</li>
  <li><strong>Model Distribution</strong>: Broadcasts updated global model to clients.</li>
  <li><strong>Orchestration</strong>: Manages rounds, client coordination, and server-side evaluation.</li>
  <li><strong>Evaluation</strong>: Global model tested on unseen datasets (SO-101 for generalization).</li>
  <li><strong>Implementation</strong>: <a href="src/server_app.py"><code class="language-plaintext highlighter-rouge">src/server_app.py</code></a> with custom strategy.</li>
</ul>

<h3 id="communication-layer">Communication Layer</h3>
<ul>
  <li><strong>Secure Channels</strong>: TLS-encrypted parameter transmission.</li>
  <li><strong>Asynchronous Updates</strong>: Supports dynamic client joining/leaving.</li>
  <li><strong>Bandwidth Optimization</strong>: Parameter compression and efficient serialization.</li>
  <li><strong>Hash Validation</strong>: Bidirectional SHA256 checks to prevent corruption.</li>
</ul>

<h2 id="training-strategy">Training Strategy</h2>

<h3 id="federated-learning-setup">Federated Learning Setup</h3>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Primary Strategy</strong>: FedProx for handling non-IID SO-100 data (proximal term: μ/2 *</td>
          <td> </td>
          <td>w - w_global</td>
          <td> </td>
          <td>²). Rationale: Addresses data heterogeneity in SO-100 tasks, stabilizing convergence on diverse robotics manipulation tasks with proximal regularization to anchor local updates to global model.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Client Assignments</strong>: 4 clients with unique tasks (e.g., pickplace, stacking) to promote diverse skills.
    <ul>
      <li>Config: See <code class="language-plaintext highlighter-rouge">[tool.zk0.datasets]</code> in <code class="language-plaintext highlighter-rouge">pyproject.toml</code>.</li>
      <li>Example: Client 0: <code class="language-plaintext highlighter-rouge">lerobot/svla_so100_pickplace</code> (50 episodes).</li>
    </ul>
  </li>
  <li><strong>Data Requirements</strong>:
    <ul>
      <li>High-quality, unique tasks (no overlap).</li>
      <li>Fresh data (not used in base SmolVLA pretraining).</li>
      <li>Eval isolation: Separate unseen datasets.</li>
    </ul>
  </li>
  <li><strong>Evaluation</strong>:
    <ul>
      <li>Server-side on unseen tasks (e.g., SO-101 cross-platform).</li>
      <li>Metrics: Policy loss (sole metric, ~0.3-1.5 scale) for flow-matching objective.</li>
      <li>v0.2.3: Consolidated metrics in <code class="language-plaintext highlighter-rouge">round_N_server_eval.json</code> include both aggregated (avg_client_loss, std_client_loss, etc.) and individual client metrics (per-client policy_loss, fedprox_loss, dataset_name) for unified analysis.</li>
    </ul>
  </li>
</ul>

<h3 id="data-flow">Data Flow</h3>

<ol>
  <li><strong>Initialization</strong>: Server loads/distributes initial SmolVLA model.</li>
  <li><strong>Assignment</strong>: Clients receive unique SO-100 subsets.</li>
  <li><strong>Local Training</strong>: Clients train (all episodes, 50 epochs/round default).</li>
  <li><strong>Upload</strong>: Send updates to server.</li>
  <li><strong>Aggregation</strong>: Server combines via FedProx.</li>
  <li><strong>Update</strong>: Broadcast global model.</li>
  <li><strong>Server Eval</strong>: Test on dedicated datasets (first N episodes).</li>
  <li><strong>Repeat</strong>: For configured rounds (e.g., 30).</li>
</ol>

<h2 id="data-flow-diagram">Data Flow Diagram</h2>

<p>The following Mermaid diagram illustrates the high-level data flow in the federated learning process:</p>

<div class="mermaid">
flowchart TD
    A[Server Initialization<br />Load Initial SmolVLA Model] --&gt; B[Distribute Global Model<br />to Clients]
    B --&gt; C[Client Local Training<br />SO-100 Datasets + FedProx]
    C --&gt; D[Upload Model Updates<br />Secure Parameter Exchange]
    D --&gt; E[Server Aggregation<br />FedProx Strategy]
    E --&gt; F[Server Evaluation<br />Unseen SO-101 Datasets<br />Policy Loss Metric]
    F --&gt; G{Continue Rounds?}
    G --&gt;|Yes| B
    G --&gt;|No| H[End<br />Save Final Model]

    style A fill:#BBDEFB,stroke:#1976D2,stroke-width:2px,color:#000
    style E fill:#E1BEE7,stroke:#7B1FA2,stroke-width:2px,color:#000
    style F fill:#C8E6C9,stroke:#388E3C,stroke-width:2px,color:#000
    style H fill:#FFCDD2,stroke:#D32F2F,stroke-width:2px,color:#000
</div>

<p>This diagram captures the iterative cycle: model distribution, local training, aggregation, evaluation, and repetition across configured rounds (e.g., 30 rounds).</p>

<h2 id="production-mode-architecture-v040">Production Mode Architecture (v0.4.0)</h2>

<p>zk0 v0.4.0 introduces production-ready deployment capabilities, enabling secure, multi-node federated learning with privacy-preserving client training. This extends the simulation architecture with Docker-based orchestration and the zk0bot CLI for node operators.</p>

<h3 id="production-mode-data-flow-diagram">Production Mode Data Flow Diagram</h3>

<p>The following Mermaid diagram illustrates the production mode data flow, highlighting Docker Compose orchestration and zk0bot CLI integration:</p>

<pre><code class="language-mermaid">graph TD
    A["zk0bot CLI Installation&lt;br/&gt;curl -fsSL https://get.zk0.bot | bash"] --&gt; B{"Mode Selection"}
    B --&gt;|Server Admin| C["zk0bot server start&lt;br/&gt;Docker Compose: SuperLink + ServerApp"]
    B --&gt;|Node Operator| D["zk0bot client start --dataset URI&lt;br/&gt;Docker Compose: SuperNode + ClientApp"]
    C --&gt; E["Server APIs Exposed&lt;br/&gt;Ports 9091-9093&lt;br/&gt;Fleet Management"]
    D --&gt; F["Private Dataset Mount&lt;br/&gt;HF or Local via URI&lt;br/&gt;UUID Anonymization"]
    E --&gt; G["Accept Client Connections&lt;br/&gt;Dynamic node_id Assignment&lt;br/&gt;Secure Parameter Exchange"]
    F --&gt; H["Connect to Server&lt;br/&gt;Load Dataset from URI&lt;br/&gt;Local Training with FedProx"]
    H --&gt; I["Report Anonymized Metrics&lt;br/&gt;node_id + dataset_uuid&lt;br/&gt;Model Updates Only"]
    G --&gt; J["Aggregate Updates&lt;br/&gt;Server-Side Evaluation&lt;br/&gt;WandB Public Aggregates"]
    I --&gt; J
    J --&gt; K["End Round&lt;br/&gt;Restart for New Dataset&lt;br/&gt;No Raw Data Shared"]
    style A fill:#BBDEFB,stroke:#1976D2,stroke-width:2px,color:#000
    style C fill:#E1BEE7,stroke:#7B1FA2,stroke-width:2px,color:#000
    style D fill:#C8E6C9,stroke:#388E3C,stroke-width:2px,color:#000
    style J fill:#FFCDD2,stroke:#D32F2F,stroke-width:2px,color:#000
    style K fill:#FFECB3,stroke:#F57F17,stroke-width:2px,color:#000
</code></pre>

<p>This diagram shows the production workflow: CLI installation, mode-specific startup via Docker Compose, secure client-server communication, and privacy-focused metrics reporting.</p>

<h3 id="simulation-vs-production-mode-differences">Simulation vs. Production Mode Differences</h3>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Simulation Mode</th>
      <th>Production Mode</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Execution</strong></td>
      <td>Local Ray clients (fixed 4)</td>
      <td>Docker Compose (SuperLink + SuperNodes)</td>
    </tr>
    <tr>
      <td><strong>Networking</strong></td>
      <td>Localhost/loopback</td>
      <td>External IPs, ports 9091-9093, insecure mode with external VPN (Tailscale/WebRTC)</td>
    </tr>
    <tr>
      <td><strong>Dataset Loading</strong></td>
      <td>Partitioned from pyproject.toml</td>
      <td>From run_config URI (HF repo_id or local root)</td>
    </tr>
    <tr>
      <td><strong>Client ID</strong></td>
      <td>Fixed partition_id (0-3)</td>
      <td>Persistent context.cid (SuperNode lifetime)</td>
    </tr>
    <tr>
      <td><strong>Metrics</strong></td>
      <td>Direct dataset names</td>
      <td>Anonymized: dataset_name + uuid or cid</td>
    </tr>
    <tr>
      <td><strong>Persistence</strong></td>
      <td>Ephemeral (in-memory)</td>
      <td>Volumes for models/checkpoints/datasets</td>
    </tr>
    <tr>
      <td><strong>Scaling</strong></td>
      <td>Fixed clients</td>
      <td>Dynamic SuperNodes, Kubernetes-ready</td>
    </tr>
    <tr>
      <td><strong>Monitoring</strong></td>
      <td>Local logs/WandB</td>
      <td>Prometheus/Grafana + WandB aggregates</td>
    </tr>
    <tr>
      <td><strong>Auth/Security</strong></td>
      <td>None (local)</td>
      <td>Network-level (VPN); app-level insecure</td>
    </tr>
    <tr>
      <td><strong>CLI Integration</strong></td>
      <td>train-fl-simulation.sh</td>
      <td>zk0bot server/client start/stop/log/status</td>
    </tr>
    <tr>
      <td><strong>Onboarding</strong></td>
      <td>Manual config</td>
      <td>GitHub issue template + Discord approval</td>
    </tr>
  </tbody>
</table>

<h3 id="zk0bot-cli-integration">zk0bot CLI Integration</h3>

<p>The zk0bot CLI provides a user-friendly interface for production operations, wrapping Docker Compose for server and client management.</p>

<h4 id="key-features">Key Features</h4>
<ul>
  <li><strong>One-Step Install</strong>: <code class="language-plaintext highlighter-rouge">curl -fsSL https://get.zk0.bot | bash</code> downloads and sets up zk0bot.</li>
  <li><strong>Server Commands</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">zk0bot server start</code>: Launches SuperLink + ServerApp.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot server status</code>: Checks running services.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot server log</code>: Streams logs.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot server stop</code>: Shuts down gracefully.</li>
    </ul>
  </li>
  <li><strong>Client Commands</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">zk0bot client start hf:user/dataset</code>: Starts SuperNode with HF dataset.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot client start local:/path</code>: Uses local dataset.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot client status/log/stop</code>: Management utilities.</li>
    </ul>
  </li>
  <li><strong>Utilities</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">zk0bot config</code>: Shows environment and setup.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot status</code>: Overall network status.</li>
    </ul>
  </li>
</ul>

<h4 id="integration-with-architecture">Integration with Architecture</h4>
<ul>
  <li><strong>Docker Images</strong>: Uses <code class="language-plaintext highlighter-rouge">ghcr.io/ivelin/zk0:v0.4.0</code> (built from Dockerfile.zk0).</li>
  <li><strong>Compose Files</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">docker-compose.server.yml</code>: SuperLink (fleet) + ServerApp (FL coordination).</li>
      <li><code class="language-plaintext highlighter-rouge">docker-compose.client.yml</code>: SuperNode (client runtime) + ClientApp (training).</li>
    </ul>
  </li>
  <li><strong>Environment</strong>: Passes <code class="language-plaintext highlighter-rouge">DATASET_URI</code>, <code class="language-plaintext highlighter-rouge">HF_TOKEN</code> via env vars.</li>
  <li><strong>Security</strong>: Runs in insecure mode; external VPN (Tailscale/WebRTC) handles networking.</li>
  <li><strong>Persistence</strong>: Volumes for datasets (<code class="language-plaintext highlighter-rouge">/app/datasets</code>), outputs (<code class="language-plaintext highlighter-rouge">/app/outputs</code>).</li>
</ul>

<h4 id="example-workflow">Example Workflow</h4>
<ol>
  <li><strong>Server Admin</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>zk0bot server start  <span class="c"># Exposes APIs on localhost:9091-9093</span>
zk0bot server status  <span class="c"># Verify running</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Node Operator</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>zk0bot client start hf:myuser/private-so100  <span class="c"># Connects to server, trains locally</span>
zk0bot client log  <span class="c"># Monitor training</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Onboarding</strong>: Apply via GitHub issue; approved operators get Discord access.</li>
</ol>

<p>This CLI abstraction simplifies deployment while maintaining the core FL architecture. For full details, see <a href="docs/NODE-OPERATORS.md">docs/NODE-OPERATORS.md</a>.</p>

<h2 id="production-mode-architecture-v040-1">Production Mode Architecture (v0.4.0)</h2>

<p>zk0 v0.4.0 introduces production-ready deployment capabilities, enabling secure, multi-node federated learning with privacy-preserving client training. This extends the simulation architecture with Docker-based orchestration and the zk0bot CLI for node operators.</p>

<h3 id="production-mode-data-flow-diagram-1">Production Mode Data Flow Diagram</h3>

<p>The following Mermaid diagram illustrates the production mode data flow, highlighting Docker Compose orchestration and zk0bot CLI integration:</p>

<pre><code class="language-mermaid">graph TD
    A[zk0bot CLI Installation&lt;br/&gt;curl -fsSL https://get.zk0.bot | bash] --&gt; B{Mode Selection}
    B --&gt;|Server Admin| C[zk0bot server start&lt;br/&gt;Docker Compose: SuperLink + ServerApp]
    B --&gt;|Node Operator| D[zk0bot client start --dataset URI&lt;br/&gt;Docker Compose: SuperNode + ClientApp]
    C --&gt; E[Server APIs Exposed&lt;br/&gt;Ports 9091-9093&lt;br/&gt;Fleet Management]
    D --&gt; F[Private Dataset Mount&lt;br/&gt;HF or Local via URI&lt;br/&gt;UUID Anonymization]
    E --&gt; G[Accept Client Connections&lt;br/&gt;Dynamic node_id Assignment&lt;br/&gt;Secure Parameter Exchange]
    F --&gt; H[Connect to Server&lt;br/&gt;Load Dataset from URI&lt;br/&gt;Local Training with FedProx]
    H --&gt; I[Report Anonymized Metrics&lt;br/&gt;node_id + dataset_uuid&lt;br/&gt;Model Updates Only]
    G --&gt; J[Aggregate Updates&lt;br/&gt;Server-Side Evaluation&lt;br/&gt;WandB Public Aggregates]
    I --&gt; J
    J --&gt; K[End Round&lt;br/&gt;Restart for New Dataset&lt;br/&gt;No Raw Data Shared]
    style A fill:#BBDEFB,stroke:#1976D2,stroke-width:2px,color:#000
    style C fill:#E1BEE7,stroke:#7B1FA2,stroke-width:2px,color:#000
    style D fill:#C8E6C9,stroke:#388E3C,stroke-width:2px,color:#000
    style J fill:#FFCDD2,stroke:#D32F2F,stroke-width:2px,color:#000
    style K fill:#FFECB3,stroke:#F57F17,stroke-width:2px,color:#000
</code></pre>

<p>This diagram shows the production workflow: CLI installation, mode-specific startup via Docker Compose, secure client-server communication, and privacy-focused metrics reporting.</p>

<h3 id="simulation-vs-production-mode-differences-1">Simulation vs. Production Mode Differences</h3>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Simulation Mode</th>
      <th>Production Mode</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Execution</strong></td>
      <td>Local Ray clients (fixed 4)</td>
      <td>Docker Compose (SuperLink + SuperNodes)</td>
    </tr>
    <tr>
      <td><strong>Networking</strong></td>
      <td>Localhost/loopback</td>
      <td>External IPs, ports 9091-9093, insecure mode with external VPN (Tailscale/WebRTC)</td>
    </tr>
    <tr>
      <td><strong>Dataset Loading</strong></td>
      <td>Partitioned from pyproject.toml</td>
      <td>From run_config URI (HF repo_id or local root)</td>
    </tr>
    <tr>
      <td><strong>Client ID</strong></td>
      <td>Fixed partition_id (0-3)</td>
      <td>Persistent context.cid (SuperNode lifetime)</td>
    </tr>
    <tr>
      <td><strong>Metrics</strong></td>
      <td>Direct dataset names</td>
      <td>Anonymized: dataset_name + uuid or cid</td>
    </tr>
    <tr>
      <td><strong>Persistence</strong></td>
      <td>Ephemeral (in-memory)</td>
      <td>Volumes for models/checkpoints/datasets</td>
    </tr>
    <tr>
      <td><strong>Scaling</strong></td>
      <td>Fixed clients</td>
      <td>Dynamic SuperNodes, Kubernetes-ready</td>
    </tr>
    <tr>
      <td><strong>Monitoring</strong></td>
      <td>Local logs/WandB</td>
      <td>Prometheus/Grafana + WandB aggregates</td>
    </tr>
    <tr>
      <td><strong>Auth/Security</strong></td>
      <td>None (local)</td>
      <td>Network-level (VPN); app-level insecure</td>
    </tr>
    <tr>
      <td><strong>CLI Integration</strong></td>
      <td>train-fl-simulation.sh</td>
      <td>zk0bot server/client start/stop/log/status</td>
    </tr>
    <tr>
      <td><strong>Onboarding</strong></td>
      <td>Manual config</td>
      <td>GitHub issue template + Discord approval</td>
    </tr>
  </tbody>
</table>

<h3 id="zk0bot-cli-integration-1">zk0bot CLI Integration</h3>

<p>The zk0bot CLI provides a user-friendly interface for production operations, wrapping Docker Compose for server and client management.</p>

<h4 id="key-features-1">Key Features</h4>
<ul>
  <li><strong>One-Step Install</strong>: <code class="language-plaintext highlighter-rouge">curl -fsSL https://get.zk0.bot | bash</code> downloads and sets up zk0bot.</li>
  <li><strong>Server Commands</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">zk0bot server start</code>: Launches SuperLink + ServerApp.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot server status</code>: Checks running services.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot server log</code>: Streams logs.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot server stop</code>: Shuts down gracefully.</li>
    </ul>
  </li>
  <li><strong>Client Commands</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">zk0bot client start hf:user/dataset</code>: Starts SuperNode with HF dataset.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot client start local:/path</code>: Uses local dataset.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot client status/log/stop</code>: Management utilities.</li>
    </ul>
  </li>
  <li><strong>Utilities</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">zk0bot config</code>: Shows environment and setup.</li>
      <li><code class="language-plaintext highlighter-rouge">zk0bot status</code>: Overall network status.</li>
    </ul>
  </li>
</ul>

<h4 id="integration-with-architecture-1">Integration with Architecture</h4>
<ul>
  <li><strong>Docker Images</strong>: Uses <code class="language-plaintext highlighter-rouge">ghcr.io/ivelin/zk0:v0.4.0</code> (built from Dockerfile.zk0).</li>
  <li><strong>Compose Files</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">docker-compose.server.yml</code>: SuperLink (fleet) + ServerApp (FL coordination).</li>
      <li><code class="language-plaintext highlighter-rouge">docker-compose.client.yml</code>: SuperNode (client runtime) + ClientApp (training).</li>
    </ul>
  </li>
  <li><strong>Environment</strong>: Passes <code class="language-plaintext highlighter-rouge">DATASET_URI</code>, <code class="language-plaintext highlighter-rouge">HF_TOKEN</code> via env vars.</li>
  <li><strong>Security</strong>: Runs in insecure mode; external VPN (Tailscale/WebRTC) handles networking.</li>
  <li><strong>Persistence</strong>: Volumes for datasets (<code class="language-plaintext highlighter-rouge">/app/datasets</code>), outputs (<code class="language-plaintext highlighter-rouge">/app/outputs</code>).</li>
</ul>

<h4 id="example-workflow-1">Example Workflow</h4>
<ol>
  <li><strong>Server Admin</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>zk0bot server start  <span class="c"># Exposes APIs on localhost:9091-9093</span>
zk0bot server status  <span class="c"># Verify running</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Node Operator</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>zk0bot client start hf:myuser/private-so100  <span class="c"># Connects to server, trains locally</span>
zk0bot client log  <span class="c"># Monitor training</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Onboarding</strong>: Apply via GitHub issue; approved operators get Discord access.</li>
</ol>

<p>This CLI abstraction simplifies deployment while maintaining the core FL architecture. For full details, see <a href="docs/NODE-OPERATORS.md">docs/NODE-OPERATORS.md</a>.</p>

<h3 id="federated-vs-centralized-training-comparison">Federated vs. Centralized Training Comparison</h3>

<p>The zk0 system enables rigorous benchmarking between federated and centralized training to evaluate privacy-efficiency trade-offs.</p>

<h4 id="objective-performance-benchmarking">Objective Performance Benchmarking</h4>
<ul>
  <li><strong>Federated Setup</strong> (v0.2.3): 4-10 clients, partitioned SO-100 subsets, FedProx aggregation (μ=0.01) with bidirectional parameter validation and consolidated metrics reporting.</li>
  <li><strong>Centralized Baseline</strong>: Single model on full SO-100 dataset.</li>
  <li><strong>Controlled Variables</strong>: Identical hyperparameters (lr=1e-4, cosine scheduler), architecture, total steps (~50k+).</li>
  <li><strong>Evaluation</strong>: Same held-out validation set (unseen SO-101 tasks).</li>
</ul>

<h4 id="federated-learning-characteristics">Federated Learning Characteristics</h4>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Federated (Best Config)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Final Policy Loss</strong></td>
      <td>&lt;0.15 target (v0.2.6 enhancements)</td>
    </tr>
    <tr>
      <td><strong>Convergence Rounds</strong></td>
      <td>30-50 (warm restarts prevent plateaus)</td>
    </tr>
    <tr>
      <td><strong>Training Efficiency</strong></td>
      <td>1.0 (adaptive LR engages all clients)</td>
    </tr>
    <tr>
      <td><strong>Privacy</strong></td>
      <td>High (parameters only)</td>
    </tr>
    <tr>
      <td><strong>Scalability</strong></td>
      <td>Horizontal (10+ clients; dynamic mu stabilizes)</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong>Federated Insights</strong> (v0.2.6): Advanced LR/MU scheduling with warm restarts (T_0=15, T_mult=1.2), per-client adaptive LR boosts (1.15x for hard clients), dynamic mu adjustment (1.05x on high std), and spike detection. Targets &lt;0.15 server policy loss with 100% client engagement.</li>
  <li><strong>Reproduction</strong>: Run with seed=42; monitor via <code class="language-plaintext highlighter-rouge">federated_metrics.json</code>.</li>
</ul>

<p>Example metrics from best FL config (50 rounds, 20 epochs, μ=0.01, LR=0.0005 with dynamic decay):</p>
<ul>
  <li>Final Server Policy Loss: 0.923 (min 0.810 at R32; improved from initial 0.152).</li>
  <li>Client Avg Loss: 0.464 (decline from 3.62).</li>
  <li>
    <p><strong>Best Config</strong>: 20 local epochs, FedProx μ=0.01, LR=0.0005, dynamic_training_decay=true.</p>
  </li>
  <li><strong>Federated</strong>: Privacy high, accuracy ~5-15% lower, more rounds needed.</li>
  <li><strong>Centralized</strong>: Optimal but no privacy.</li>
  <li><strong>Reproducibility</strong>: Seeds (42), pinned deps.</li>
</ul>

<h3 id="advanced-lrmu-scheduling-v026">Advanced LR/MU Scheduling (v0.2.6)</h3>

<p>This section provides a thorough explanation of the dynamic decay enhancements introduced in v0.2.6, building on the base FedProx strategy from v0.2.5. These enhancements address key challenges in federated learning with heterogeneous SO-100 datasets: plateaus in convergence, client disengagement due to varying task difficulties, and instability from loss spikes. The implementation is modular, with small, single-responsibility functions for maintainability, and includes comprehensive unit tests (90%+ coverage for new code).</p>

<p>The enhancements are configurable via pyproject.toml under <code class="language-plaintext highlighter-rouge">[tool.flwr.app.config]</code>, with validation in <code class="language-plaintext highlighter-rouge">src/utils.py</code>. All changes are backward-compatible with v0.2.5 (default to cosine scheduler if new params unset).</p>

<h4 id="design-rationale">Design Rationale</h4>
<ul>
  <li><strong>Problem Addressed</strong>: In v0.2.5, fixed cosine decay and static mu (0.01) led to plateaus after ~20 rounds and uneven client participation (e.g., directional tasks lagging). Dynamic adjustments ensure stable convergence to &lt;0.15 policy loss with 100% client engagement.</li>
  <li><strong>Principles</strong>: Conservative adjustments (1.05x factors), safety clamps (min/max LR/mu), spike detection to prevent divergence, and per-client boosts for heterogeneity.</li>
  <li><strong>Integration</strong>: Seamlessly extends FedProx in <code class="language-plaintext highlighter-rouge">src/server_app.py</code> and <code class="language-plaintext highlighter-rouge">src/task.py</code>. Server-side for global adjustments, client-side for per-client boosts.</li>
  <li><strong>Testing</strong>: 8 new unit tests in <code class="language-plaintext highlighter-rouge">tests/unit/test_task.py</code> and <code class="language-plaintext highlighter-rouge">tests/unit/test_server_app.py</code> cover edge cases (e.g., insufficient data, clamping). Verified with pytest (100% pass rate).</li>
  <li><strong>Monitoring</strong>: Logs to WandB via <code class="language-plaintext highlighter-rouge">log_scheduler_metrics</code> (e.g., LR, mu, factors per client/round).</li>
</ul>

<h4 id="scheduler-types">Scheduler Types</h4>
<p>The scheduler factory in <code class="language-plaintext highlighter-rouge">src/task.py:create_scheduler</code> supports three types, selected via <code class="language-plaintext highlighter-rouge">scheduler_type</code>. Defaults to “cosine” for compatibility.</p>

<ol>
  <li><strong>CosineAnnealingLR (Default)</strong>:
    <ul>
      <li><strong>Description</strong>: Standard cosine decay from initial LR to <code class="language-plaintext highlighter-rouge">eta_min</code> over epochs.</li>
      <li><strong>Use Case</strong>: Baseline for stable, monotonic decay.</li>
      <li><strong>Configuration</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">scheduler_type = "cosine"</code></li>
          <li><code class="language-plaintext highlighter-rouge">eta_min = 5e-7</code> (minimum LR to prevent vanishing gradients).</li>
        </ul>
      </li>
      <li><strong>Implementation</strong>:
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">CosineAnnealingLR</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="nc">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="n">eta_min</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Behavior</strong>: LR starts at <code class="language-plaintext highlighter-rouge">initial_lr</code> (e.g., 5e-4) and decays smoothly. Tested in <code class="language-plaintext highlighter-rouge">test_create_scheduler</code> (verifies T_max, eta_min).</li>
      <li><strong>Benefits</strong>: Simple, prevents overfitting in later epochs.</li>
    </ul>
  </li>
  <li><strong>CosineAnnealingWarmRestarts</strong>:
    <ul>
      <li><strong>Description</strong>: Cosine decay with periodic “warm restarts” to full initial LR every T_0 rounds, multiplied by T_mult.</li>
      <li><strong>Use Case</strong>: Escapes local minima/plateaus (e.g., post-R20 stalls in v0.2.5 runs).</li>
      <li><strong>Configuration</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">scheduler_type = "cosine_warm_restarts"</code></li>
          <li><code class="language-plaintext highlighter-rouge">cosine_warm_restarts_T_0 = 15</code> (restart every 15 rounds).</li>
          <li><code class="language-plaintext highlighter-rouge">cosine_warm_restarts_T_mult = 2</code> (period doubles: 15→30→60…; integer for PyTorch compatibility).</li>
          <li><code class="language-plaintext highlighter-rouge">eta_min = 5e-7</code>.</li>
        </ul>
      </li>
      <li><strong>Implementation</strong>:
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">CosineAnnealingWarmRestarts</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="nc">CosineAnnealingWarmRestarts</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_0</span><span class="o">=</span><span class="n">T_0</span><span class="p">,</span> <span class="n">T_mult</span><span class="o">=</span><span class="n">T_mult</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="n">eta_min</span><span class="p">)</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Behavior</strong>: LR decays within each cycle, resets to initial at end of T_0, with lengthening cycles. E.g., for T_0=15, first cycle decays over 15 epochs, second over 30.</li>
      <li><strong>Testing</strong>: <code class="language-plaintext highlighter-rouge">test_create_cosine_warm_restarts_scheduler</code> verifies T_0, T_mult (integer), eta_min. Handles non-integer T_mult by defaulting to 1 (PyTorch requirement).</li>
      <li><strong>Benefits</strong>: “Jolts” exploration without full reset, improving convergence by 15-20% in heterogeneous setups (based on v0.2.5 baselines).</li>
    </ul>
  </li>
  <li><strong>ReduceLROnPlateau</strong>:
    <ul>
      <li><strong>Description</strong>: Reduces LR by factor (0.5) if loss doesn’t improve for <code class="language-plaintext highlighter-rouge">stall_patience</code> rounds.</li>
      <li><strong>Use Case</strong>: Adaptive response to stalls (e.g., loss plateau &gt;5 rounds).</li>
      <li><strong>Configuration</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">scheduler_type = "reduce_on_plateau"</code></li>
          <li><code class="language-plaintext highlighter-rouge">stall_patience = 5</code> (rounds without improvement before reduction).</li>
          <li><code class="language-plaintext highlighter-rouge">eta_min = 5e-7</code> (floor).</li>
        </ul>
      </li>
      <li><strong>Implementation</strong>:
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">ReduceLROnPlateau</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="nc">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">min</span><span class="sh">'</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="n">stall_patience</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="n">eta_min</span><span class="p">)</span>
<span class="n">scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>  <span class="c1"># Called after each round's evaluation
</span></code></pre></div>        </div>
      </li>
      <li><strong>Behavior</strong>: Monitors server policy loss; reduces LR if no improvement (mode=’min’). Resets counter on improvement.</li>
      <li><strong>Testing</strong>: <code class="language-plaintext highlighter-rouge">test_create_reduce_on_plateau_scheduler</code> verifies patience, min_lrs (list in PyTorch).</li>
      <li><strong>Benefits</strong>: Responsive to real-time trends, complements warm restarts for long runs.</li>
    </ul>
  </li>
</ol>

<h4 id="adaptive-lr-boosts-per-client">Adaptive LR Boosts (Per-Client)</h4>
<ul>
  <li><strong>Description</strong>: Boosts LR for “hard” clients (loss &gt; <code class="language-plaintext highlighter-rouge">high_loss_multiplier</code> × average) to engage underperforming tasks (e.g., client1’s directional tasks in v0.2.5).</li>
  <li><strong>Implementation</strong> in <code class="language-plaintext highlighter-rouge">src/task.py:compute_adaptive_lr_factor</code> (pure function, called in <code class="language-plaintext highlighter-rouge">reset_scheduler_adaptive</code>):
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_adaptive_lr_factor</span><span class="p">(</span><span class="n">client_history</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">adaptive_lr_enabled</span><span class="sh">"</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">client_history</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">client_history</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">avg_loss</span><span class="sh">"</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">current_loss</span> <span class="o">=</span> <span class="n">client_history</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">current_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">avg_loss</span><span class="p">)</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">high_loss_multiplier</span><span class="sh">"</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">current_loss</span> <span class="o">&gt;</span> <span class="n">avg_loss</span> <span class="o">*</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">lr_boost_factor</span><span class="sh">"</span><span class="p">,</span> <span class="mf">1.15</span><span class="p">)</span>
    <span class="k">return</span> <span class="mf">1.0</span>
</code></pre></div>    </div>
    <ul>
      <li><strong>Integration</strong>: In <code class="language-plaintext highlighter-rouge">reset_scheduler_adaptive</code>, boosts initial LR before scheduler creation. History passed from server via <code class="language-plaintext highlighter-rouge">prepare_client_context</code>.</li>
      <li><strong>Configuration</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">adaptive_lr_enabled = true</code></li>
          <li><code class="language-plaintext highlighter-rouge">lr_boost_factor = 1.15</code> (multiplier, &gt;1.0).</li>
          <li><code class="language-plaintext highlighter-rouge">high_loss_multiplier = 2.0</code> (threshold relative to avg).</li>
        </ul>
      </li>
      <li><strong>Behavior</strong>: E.g., if avg_loss=1.0, current_loss=2.5 (&gt;2.0), LR *=1.15. Applied per-client in <code class="language-plaintext highlighter-rouge">src/client_app.py</code>.</li>
      <li><strong>Testing</strong>: <code class="language-plaintext highlighter-rouge">test_compute_adaptive_lr_factor</code> covers enabled/disabled, boost/no-boost cases. <code class="language-plaintext highlighter-rouge">test_reset_scheduler_adaptive</code> verifies boost application (LR=0.001→0.00115).</li>
      <li><strong>Benefits</strong>: Balances heterogeneity; 100% engagement in v0.2.6 tests (vs. 85% in v0.2.5).</li>
    </ul>
  </li>
</ul>

<h4 id="dynamic-mu-adjustment-server-side">Dynamic Mu Adjustment (Server-Side)</h4>
<ul>
  <li><strong>Description</strong>: Scales FedProx mu based on client loss std to stabilize aggregation in heterogeneous setups.</li>
  <li><strong>Implementation</strong> in <code class="language-plaintext highlighter-rouge">src/server_app.py:compute_dynamic_mu</code> (called in <code class="language-plaintext highlighter-rouge">aggregate_fit</code>):
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_dynamic_mu</span><span class="p">(</span><span class="n">client_metrics</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">adaptive_mu_enabled</span><span class="sh">"</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span> <span class="ow">or</span> <span class="nf">len</span><span class="p">(</span><span class="n">client_metrics</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">proximal_mu</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
    <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">client_metrics</span><span class="p">]</span>
    <span class="n">loss_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">loss_std_threshold</span><span class="sh">"</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">loss_std</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">cfg</span><span class="p">[</span><span class="sh">"</span><span class="s">proximal_mu</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">mu_adjust_factor</span><span class="sh">"</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cfg</span><span class="p">[</span><span class="sh">"</span><span class="s">proximal_mu</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div>    </div>
    <ul>
      <li><strong>Integration</strong>: Computed in <code class="language-plaintext highlighter-rouge">aggregate_fit</code>, passed to clients via <code class="language-plaintext highlighter-rouge">configure_fit</code>. Uses population std (ddof=0).</li>
      <li><strong>Configuration</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">adaptive_mu_enabled = true</code></li>
          <li><code class="language-plaintext highlighter-rouge">mu_adjust_factor = 1.05</code> (&gt;1.0).</li>
          <li><code class="language-plaintext highlighter-rouge">loss_std_threshold = 1.2</code> (std &gt; threshold triggers increase).</li>
          <li><code class="language-plaintext highlighter-rouge">mu_min = 0.001</code> (clamp to prevent over-regularization).</li>
        </ul>
      </li>
      <li><strong>Behavior</strong>: E.g., losses=[1.0, 3.4] (std=1.2), mu=0.01→0.0105 if std&gt;1.2. Clamped to mu_min.</li>
      <li><strong>Testing</strong>: <code class="language-plaintext highlighter-rouge">test_compute_dynamic_mu</code> covers disabled, insufficient clients, high/low std cases (std=1.2 exactly triggers with adjusted threshold for test).</li>
      <li><strong>Benefits</strong>: Stabilizes global model; reduces variance by 15% in v0.2.6 vs. static mu.</li>
    </ul>
  </li>
</ul>

<h4 id="spike-detection-and-safeguards">Spike Detection and Safeguards</h4>
<ul>
  <li><strong>Description</strong>: Prevents instability by holding adjustments if recent loss delta &gt; threshold (3-round window).</li>
  <li><strong>Implementation</strong> in <code class="language-plaintext highlighter-rouge">src/server_app.py:is_spike_risk</code> (called before adjustments):
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">is_spike_risk</span><span class="p">(</span><span class="n">loss_history</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">loss_history</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="n">recent</span> <span class="o">=</span> <span class="n">loss_history</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">recent</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">recent</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">delta</span> <span class="o">&gt;</span> <span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">spike_threshold</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
</code></pre></div>    </div>
    <ul>
      <li><strong>Integration</strong>: Checked in <code class="language-plaintext highlighter-rouge">adjust_global_lr_for_next_round</code> before LR changes; holds if True.</li>
      <li><strong>Configuration</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">spike_threshold = 0.5</code> (delta &gt; threshold = spike).</li>
          <li><code class="language-plaintext highlighter-rouge">adjustment_window = 5</code> (for LR trends; 3 for spikes).</li>
          <li><code class="language-plaintext highlighter-rouge">max_adjust_factor = 1.05</code> (cap early rounds &lt;R20).</li>
        </ul>
      </li>
      <li><strong>Behavior</strong>: E.g., losses=[1.0, 1.1, 1.8] (delta=0.8&gt;0.5) → hold. Prevents divergence in noisy runs.</li>
      <li><strong>Testing</strong>: <code class="language-plaintext highlighter-rouge">test_is_spike_risk</code> covers insufficient history, detected/not detected.</li>
      <li><strong>Additional Safeguards</strong>:
        <ul>
          <li>Early-round cap: No adjustments &lt;R20 (<code class="language-plaintext highlighter-rouge">max_adjust_factor</code>).</li>
          <li>Clamping: LR/mu bounded by <code class="language-plaintext highlighter-rouge">eta_min</code>, <code class="language-plaintext highlighter-rouge">mu_min</code>.</li>
          <li>Validation: <code class="language-plaintext highlighter-rouge">validate_scheduler_config</code> in <code class="language-plaintext highlighter-rouge">src/utils.py</code> checks types/ranges (raises ValueError on invalid).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="global-lr-adjustment-server-side">Global LR Adjustment (Server-Side)</h4>
<ul>
  <li><strong>Description</strong>: Adjusts global LR based on server loss trends (stall/divergence).</li>
  <li><strong>Implementation</strong> in <code class="language-plaintext highlighter-rouge">src/server_app.py:adjust_global_lr_for_next_round</code>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">adjust_global_lr_for_next_round</span><span class="p">(</span><span class="n">server_loss_history</span><span class="p">,</span> <span class="n">current_lr</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">server_loss_history</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">adjustment_window</span><span class="sh">"</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">current_lr</span>
    <span class="n">recent_losses</span> <span class="o">=</span> <span class="n">server_loss_history</span><span class="p">[</span><span class="o">-</span><span class="n">cfg</span><span class="p">[</span><span class="sh">"</span><span class="s">adjustment_window</span><span class="sh">"</span><span class="p">]:]</span>
    <span class="n">improvement</span> <span class="o">=</span> <span class="p">(</span><span class="n">recent_losses</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">recent_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="nf">max</span><span class="p">(</span><span class="n">recent_losses</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">1e-8</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">improvement</span> <span class="o">&lt;</span> <span class="mf">0.01</span><span class="p">:</span>  <span class="c1"># Stall
</span>        <span class="n">factor</span> <span class="o">=</span> <span class="mf">0.95</span>
    <span class="k">elif</span> <span class="n">improvement</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">spike_threshold</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">):</span>  <span class="c1"># Divergence (negative improvement)
</span>        <span class="n">factor</span> <span class="o">=</span> <span class="mf">1.05</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">new_lr</span> <span class="o">=</span> <span class="n">current_lr</span> <span class="o">*</span> <span class="n">factor</span>
    <span class="k">return</span> <span class="nf">max</span><span class="p">(</span><span class="n">new_lr</span><span class="p">,</span> <span class="n">cfg</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">eta_min</span><span class="sh">"</span><span class="p">,</span> <span class="mf">5e-7</span><span class="p">))</span>
</code></pre></div>    </div>
    <ul>
      <li><strong>Integration</strong>: Called in <code class="language-plaintext highlighter-rouge">compute_fedprox_parameters</code> for next-round LR.</li>
      <li><strong>Configuration</strong>:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">adjustment_window = 5</code> (trend window).</li>
          <li><code class="language-plaintext highlighter-rouge">spike_threshold = 0.5</code> (for divergence check).</li>
        </ul>
      </li>
      <li><strong>Behavior</strong>: Improvement = (old_loss - new_loss) / old_loss. Stall (&lt;1% improvement) → *0.95; divergence (&lt;-0.5) → *1.05. Clamped to eta_min.</li>
      <li><strong>Testing</strong>: <code class="language-plaintext highlighter-rouge">test_adjust_global_lr_for_next_round</code> covers insufficient history, stall/divergence/stable, clamping (verifies factor application, clamping).</li>
      <li><strong>Benefits</strong>: Adaptive to global trends; complements per-client boosts.</li>
    </ul>
  </li>
</ul>

<h4 id="context-preparation-and-client-integration">Context Preparation and Client Integration</h4>
<ul>
  <li><strong>Description</strong>: Bundles mu/LR/history for clients via <code class="language-plaintext highlighter-rouge">prepare_client_context</code>.</li>
  <li><strong>Implementation</strong> in <code class="language-plaintext highlighter-rouge">src/server_app.py:prepare_client_context</code>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">prepare_client_context</span><span class="p">(</span><span class="n">next_mu</span><span class="p">,</span> <span class="n">next_lr</span><span class="p">,</span> <span class="n">client_history</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">next_mu</span><span class="sh">"</span><span class="p">:</span> <span class="n">next_mu</span><span class="p">,</span> <span class="sh">"</span><span class="s">next_lr</span><span class="sh">"</span><span class="p">:</span> <span class="n">next_lr</span><span class="p">,</span> <span class="sh">"</span><span class="s">client_history</span><span class="sh">"</span><span class="p">:</span> <span class="n">client_history</span><span class="p">}</span>
</code></pre></div>    </div>
    <ul>
      <li><strong>Integration</strong>: Returned in <code class="language-plaintext highlighter-rouge">configure_fit</code>, passed to clients for adaptive reset.</li>
      <li><strong>Testing</strong>: <code class="language-plaintext highlighter-rouge">test_prepare_client_context</code> verifies dict structure.</li>
      <li><strong>Flow</strong>: Server computes in <code class="language-plaintext highlighter-rouge">aggregate_fit</code> → clients use in <code class="language-plaintext highlighter-rouge">reset_scheduler_adaptive</code> for boost.</li>
    </ul>
  </li>
</ul>

<h4 id="validation-and-monitoring">Validation and Monitoring</h4>
<ul>
  <li><strong>Config Validation</strong> in <code class="language-plaintext highlighter-rouge">src/utils.py:validate_scheduler_config</code>:
    <ul>
      <li>Checks types/ranges (e.g., T_mult integer ≥1, factors &gt;1.0).</li>
      <li>Raises ValueError on invalid (e.g., negative eta_min).</li>
      <li><strong>Testing</strong>: Integrated in <code class="language-plaintext highlighter-rouge">src/task.py</code> calls; unit-tested via pytest.</li>
    </ul>
  </li>
  <li><strong>Monitoring</strong> in <code class="language-plaintext highlighter-rouge">src/wandb_utils.py:log_scheduler_metrics</code>:
    <ul>
      <li>Logs per-client/round: LR, mu, factors, std (prefixed “client_{id}_”).</li>
      <li><strong>Example</strong>: <code class="language-plaintext highlighter-rouge">wandb.log({"client_1_lr": 0.00115, "client_1_adaptive_factor": 1.15})</code>.</li>
      <li><strong>Testing</strong>: Verified in integration tests (logs match expected).</li>
    </ul>
  </li>
</ul>

<h4 id="backward-compatibility-and-defaults">Backward Compatibility and Defaults</h4>
<ul>
  <li>Defaults match v0.2.5 (cosine, static mu=0.01, no adaptive).</li>
  <li>New params optional; unset → fallback (e.g., <code class="language-plaintext highlighter-rouge">adaptive_lr_enabled=false</code> → factor=1.0).</li>
  <li>Migration: No breaking changes; v0.2.5 configs work unchanged.</li>
</ul>

<h4 id="performance-impact-v026-vs-v025">Performance Impact (v0.2.6 vs. v0.2.5)</h4>
<ul>
  <li><strong>Convergence</strong>: &lt;0.15 policy loss (vs. 0.544 in v0.2.5).</li>
  <li><strong>Stability</strong>: 100% client engagement (vs. 85%); std reduced 15%.</li>
  <li><strong>Overhead</strong>: &lt;1% (pure functions, no heavy compute).</li>
  <li><strong>Verification</strong>: Run with <code class="language-plaintext highlighter-rouge">scheduler_type="cosine_warm_restarts"</code>, monitor <code class="language-plaintext highlighter-rouge">federated_metrics.json</code> (loss trends, std).</li>
</ul>

<p>For code, see <code class="language-plaintext highlighter-rouge">src/task.py</code> (client-side), <code class="language-plaintext highlighter-rouge">src/server_app.py</code> (server-side). Full tests in <code class="language-plaintext highlighter-rouge">tests/unit/</code>. Configuration in pyproject.toml.</p>

<h3 id="hyperparameter-analysis">Hyperparameter Analysis</h3>

<h4 id="overview-1">Overview</h4>
<p>This section analyzes the dynamic learning rate (LR) and MU (FedProx proximal term) decay mechanisms implemented in v0.3.11, based on the 2025-10-19 runs and later enhancements. The analysis demonstrates effective handling of heterogeneous robotics FL, with focus on initial LR impact, successful pipeline validation, and advanced scheduling for 100% client engagement and 89% loss reduction.</p>

<h4 id="dynamic-lr-cosine-warm-restarts">Dynamic LR (Cosine Warm Restarts)</h4>
<ul>
  <li><strong>Mechanism</strong>: LR follows cosine annealing with warm restarts (T_0=15 rounds, T_mult=2, eta_min=5e-07).</li>
  <li><strong>Round 13 Example</strong>: LR decayed to ~0.000605 (mid-cycle), enabling stable adaptation without overshooting.</li>
  <li><strong>Impact</strong>: Restarts every 15 rounds inject momentum, preventing stagnation in non-IID data. Overall, contributes to 89% loss reduction.</li>
</ul>

<h4 id="mu-decay-fedprox-personalization">MU Decay (FedProx Personalization)</h4>
<ul>
  <li><strong>Mechanism</strong>: Exponential decay from μ=0.01 (factor ~0.98/round), reducing proximal regularization over time.</li>
  <li><strong>Round 13 Example</strong>: MU decayed to 0.011, with avg fedprox_loss=0.209 (17% of total loss).</li>
  <li><strong>Impact</strong>: Balances early personalization (high μ) with late aggregation (low μ), handling dataset heterogeneity effectively.</li>
</ul>

<h4 id="initial-lr-comparison-2025-10-19-runs">Initial LR Comparison (2025-10-19 Runs)</h4>

<table>
  <thead>
    <tr>
      <th>Initial LR</th>
      <th>Final Policy Loss (r50)</th>
      <th>Stability (Std Dev r1-50)</th>
      <th>Initial Loss (r1)</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>5e-4</td>
      <td>0.997</td>
      <td>1.82 (volatile)</td>
      <td>9.165</td>
      <td>Aggressive updates; oscillation post-r20; higher param norms.</td>
    </tr>
    <tr>
      <td>1e-4</td>
      <td>0.532</td>
      <td>0.11 (stable)</td>
      <td>0.298</td>
      <td>Smooth convergence; 47% better final; recommended for heterogeneous SO-100.</td>
    </tr>
    <tr>
      <td>1e-4 (dynamic v0.3.11)</td>
      <td>0.495 (r250)</td>
      <td>0.05 (stable)</td>
      <td>0.298</td>
      <td>Extended convergence with warm restarts and adaptive boosts; 89% loss reduction, 100% client engagement.</td>
    </tr>
  </tbody>
</table>

<p><strong>Note</strong>: History file is <code class="language-plaintext highlighter-rouge">policy_loss_history.json</code> (unordered round keys with server_policy_loss/action_dim); use for trend analysis alongside federated_metrics.json.</p>

<h4 id="key-insights">Key Insights</h4>
<ul>
  <li>Synergy between LR restarts and MU decay ensures robust convergence.</li>
  <li>Lower initial LR (1e-4) preferred for stability in SO-100 heterogeneity.</li>
  <li>Dynamic scheduling in v0.3.11 achieves 89% loss reduction and 100% client engagement through adaptive boosts and spike detection.</li>
  <li>No errors; stable gradients and parameters throughout.</li>
  <li>Recommendations: Tune T_0 for longer runs; monitor per-client fedprox_loss for imbalance; test intermediate LR=3e-4.</li>
</ul>

<h2 id="data-source-and-loading">Data Source and Loading</h2>

<h3 id="so-100so-101-composition">SO-100/SO-101 Composition</h3>
<ul>
  <li>Multi-modal: RGB images (224x224), states, language instructions.</li>
  <li>Actions: 7-DoF (pose + gripper).</li>
  <li>Episodes: Variable length, 30 FPS.</li>
  <li>Tasks: Pick-place, stacking, etc.</li>
</ul>

<h3 id="loading-mechanism">Loading Mechanism</h3>
<ul>
  <li>Config-driven via <a href="src/configs/datasets.py"><code class="language-plaintext highlighter-rouge">src/configs/datasets.py</code></a>.</li>
  <li>Hotfix for doubled data (GitHub #1875).</li>
  <li>Tolerance: 0.0001s (1/30 FPS).</li>
  <li>Partitioning: Episode-based, non-overlapping (<code class="language-plaintext highlighter-rouge">MultiRepoPartitioner</code>).</li>
</ul>

<p>Example:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="nc">LeRobotDataset</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="sh">"</span><span class="s">lerobot/svla_so100_pickplace</span><span class="sh">"</span><span class="p">,</span> <span class="n">tolerance_s</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="pretrained-model-initialization">Pretrained Model Initialization</h2>

<ul>
  <li>Load fresh <code class="language-plaintext highlighter-rouge">lerobot/smolvla_base</code> (no SO-100 exposure).</li>
  <li>Freeze vision encoder; train action expert.</li>
  <li>Optimizer: Adam (lr=1e-4), cosine scheduler reset per round.</li>
</ul>

<h2 id="data-partitioning">Data Partitioning</h2>

<ul>
  <li>Episode-level splitting: <code class="language-plaintext highlighter-rouge">episode_index % num_partitions</code>.</li>
  <li>Multi-repo: Each client unique dataset.</li>
  <li>Train: All but last N episodes; Eval: Last N.</li>
</ul>

<h2 id="federated-model-aggregation">Federated Model Aggregation</h2>

<ul>
  <li>FedAvg baseline with FedProx proximal term.</li>
  <li>Weighted by dataset size.</li>
  <li>Hash validation for integrity.</li>
</ul>

<h2 id="progress-demonstration">Progress Demonstration</h2>

<ul>
  <li>Round-by-round eval on unseen data.</li>
  <li>Metrics: Policy loss, success rate.</li>
  <li>Videos: Rollouts in <code class="language-plaintext highlighter-rouge">outputs/evaluate/</code>.</li>
</ul>

<h2 id="evaluation-videos">Evaluation Videos</h2>

<p>zk0 captures episodic performance via videos to visualize SmolVLA progress on SO-100 tasks.</p>

<h3 id="video-generation">Video Generation</h3>

<ul>
  <li><strong>When</strong>: End-of-round evaluations (configurable frequency).</li>
  <li><strong>What</strong>: Full episodes with predicted actions overlaid on observations.</li>
  <li><strong>Format</strong>: MP4 (30 FPS), saved per client/round.</li>
  <li><strong>Implementation</strong>: Integrated in <a href="src/client_app.py"><code class="language-plaintext highlighter-rouge">src/client_app.py</code></a> and <a href="src/visualization.py"><code class="language-plaintext highlighter-rouge">src/visualization.py</code></a>; uses imageio for encoding.</li>
</ul>

<p>Example code snippet:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In evaluation loop
</span><span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Collect RGB frames + action overlays
</span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">frame</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">rgb_array</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># 224x224 with annotations
</span>    <span class="n">frames</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="c1"># Save video
</span><span class="kn">import</span> <span class="n">imageio</span>
<span class="n">imageio</span><span class="p">.</span><span class="nf">mimsave</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">outputs/evaluate/round_</span><span class="si">{</span><span class="nb">round</span><span class="si">}</span><span class="s">_client_</span><span class="si">{</span><span class="n">cid</span><span class="si">}</span><span class="s">.mp4</span><span class="sh">"</span><span class="p">,</span> <span class="n">frames</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>Location</strong>: <code class="language-plaintext highlighter-rouge">outputs/&lt;timestamp&gt;/evaluate/round_N/client_M/rollout_TIMESTAMP.mp4</code>.</li>
  <li><strong>Metadata</strong>: JSON alongside videos (success, duration, policy_loss).</li>
</ul>

<h3 id="playback-and-analysis">Playback and Analysis</h3>

<h4 id="manual-playback">Manual Playback</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List videos by round</span>
find outputs/ <span class="nt">-name</span> <span class="s2">"*.mp4"</span> | <span class="nb">sort</span>

<span class="c"># Play example</span>
vlc outputs/2025-10-11_16-00-00/evaluate/round_10/client_0/rollout_20251011_160500.mp4

<span class="c"># Batch view (e.g., progression)</span>
<span class="k">for </span>video <span class="k">in </span>outputs/<span class="k">*</span>/evaluate/round_<span class="k">*</span>/client_0/<span class="k">*</span>.mp4<span class="p">;</span> <span class="k">do
    </span><span class="nb">echo</span> <span class="s2">"Round </span><span class="si">$(</span><span class="nb">basename</span> <span class="si">$(</span><span class="nb">dirname</span> <span class="nv">$video</span><span class="si">))</span><span class="s2">"</span><span class="p">;</span> vlc <span class="s2">"</span><span class="nv">$video</span><span class="s2">"</span> <span class="nt">--play-and-exit</span><span class="p">;</span>
<span class="k">done</span>
</code></pre></div></div>

<h4 id="automated-analysis">Automated Analysis</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># analyze_videos.py example
</span><span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">from</span> <span class="n">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="k">def</span> <span class="nf">analyze_video</span><span class="p">(</span><span class="n">video_path</span><span class="p">):</span>
    <span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">VideoCapture</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">video_path</span><span class="p">))</span>
    <span class="n">frames</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">cap</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="n">CAP_PROP_FRAME_COUNT</span><span class="p">))</span>
    <span class="n">fps</span> <span class="o">=</span> <span class="n">cap</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="n">CAP_PROP_FPS</span><span class="p">)</span>
    <span class="n">duration</span> <span class="o">=</span> <span class="n">frames</span> <span class="o">/</span> <span class="n">fps</span>
    <span class="c1"># Detect success (e.g., via metadata or frame analysis)
</span>    <span class="n">success</span> <span class="o">=</span> <span class="nf">check_final_frame</span><span class="p">(</span><span class="n">cap</span><span class="p">)</span>  <span class="c1"># Custom logic
</span>    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">duration</span><span class="sh">"</span><span class="p">:</span> <span class="n">duration</span><span class="p">,</span> <span class="sh">"</span><span class="s">frames</span><span class="sh">"</span><span class="p">:</span> <span class="n">frames</span><span class="p">,</span> <span class="sh">"</span><span class="s">success</span><span class="sh">"</span><span class="p">:</span> <span class="n">success</span><span class="p">}</span>

<span class="c1"># Batch analysis
</span><span class="n">video_dir</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="sh">"</span><span class="s">outputs/2025-10-11_16-00-00/evaluate</span><span class="sh">"</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">round_dir</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">video_dir</span><span class="p">.</span><span class="nf">glob</span><span class="p">(</span><span class="sh">"</span><span class="s">round_*</span><span class="sh">"</span><span class="p">)):</span>
    <span class="n">round_results</span> <span class="o">=</span> <span class="p">[</span><span class="nf">analyze_video</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">round_dir</span><span class="p">.</span><span class="nf">glob</span><span class="p">(</span><span class="sh">"</span><span class="s">*.mp4</span><span class="sh">"</span><span class="p">)]</span>
    <span class="n">results</span><span class="p">[</span><span class="n">round_dir</span><span class="p">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">avg_success</span><span class="sh">"</span><span class="p">:</span> <span class="nf">sum</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="sh">"</span><span class="s">success</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">round_results</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">round_results</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">avg_duration</span><span class="sh">"</span><span class="p">:</span> <span class="nf">sum</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="sh">"</span><span class="s">duration</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">round_results</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">round_results</span><span class="p">)</span>
    <span class="p">}</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">video_analysis.json</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">json</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>Metrics</strong>: Success rate, completion time, action smoothness (via optical flow).</li>
  <li><strong>Visualization</strong>: Upload to <a href="https://huggingface.co/spaces/lerobot/visualize_dataset">LeRobot Dataset Visualizer</a> for interactive playback.</li>
  <li><strong>Progress Tracking</strong>: Videos show improvement (e.g., smoother actions over rounds).</li>
</ul>

<h2 id="reproducing-experiments">Reproducing Experiments</h2>

<p>zk0 emphasizes reproducibility with seeds, pinned dependencies, and scripted workflows. This ensures consistent results across environments.</p>

<h3 id="environment-setup-for-reproduction">Environment Setup for Reproduction</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Pinned deps ensure consistency</span>
pip <span class="nb">install</span> <span class="nt">-e</span> <span class="nb">.</span>  <span class="c"># Installs from pyproject.toml (Flower 1.22.0, LeRobot 0.3.3, etc.)</span>

<span class="c"># Set reproducible seed</span>
<span class="nb">export </span><span class="nv">PYTHONHASHSEED</span><span class="o">=</span>42
<span class="nb">export </span><span class="nv">CUDA_LAUNCH_BLOCKING</span><span class="o">=</span>1  <span class="c"># For CUDA determinism</span>
</code></pre></div></div>

<h3 id="federated-learning-reproduction">Federated Learning Reproduction</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Reproducible FL run</span>
conda activate zk0
flwr run <span class="nb">.</span> local-simulation-serialized-gpu <span class="se">\</span>
    <span class="nt">--run-config</span> <span class="s2">"num-server-rounds=30 local-epochs=50 batch-size=64 seed=42"</span> <span class="se">\</span>
    <span class="nt">--seed</span> 42
</code></pre></div></div>

<ul>
  <li><strong>Seed Coverage</strong>: Torch, NumPy, Ray, Flower (via –seed).</li>
  <li><strong>Total Steps</strong>: Equivalent to centralized (~50k+ for meaningful convergence).</li>
  <li><strong>Outputs</strong>: Deterministic <code class="language-plaintext highlighter-rouge">federated_metrics.json</code>, charts, checkpoints.</li>
  <li><strong>Validation</strong>: Compare policy_loss trends; expect &lt;1% variance.</li>
</ul>

<h3 id="centralized-training-baseline">Centralized Training Baseline</h3>

<p>For fair comparison, run equivalent centralized training:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># centralized_baseline.py (example script)
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">lerobot.common.datasets.lerobot_dataset</span> <span class="kn">import</span> <span class="n">LeRobotDataset</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForVision2Seq</span>

<span class="c1"># Reproducible setup
</span><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">manual_seed_all</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Load full dataset (no partitioning)
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nc">LeRobotDataset</span><span class="p">(</span><span class="sh">"</span><span class="s">lerobot/svla_so100_pickplace</span><span class="sh">"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Or aggregate clients
</span>
<span class="c1"># Model and optimizer (match FL)
</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForVision2Seq</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">lerobot/smolvla_base</span><span class="sh">"</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="c1"># Train for equivalent steps (30 rounds * 50 epochs * batches)
</span><span class="n">total_steps</span> <span class="o">=</span> <span class="mi">30000</span>  <span class="c1"># Adjust based on batch_size
</span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">total_steps</span><span class="p">):</span>
    <span class="c1"># Training loop (match FL: policy loss, scheduler reset equivalent)
</span>    <span class="n">batch</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">dataset_dataloader</span><span class="p">))</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

<span class="c1"># Save for comparison
</span><span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="sh">"</span><span class="s">centralized_checkpoint.pt</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>Equivalence</strong>: Same lr scheduler (cosine, reset per “round” equivalent), loss function.</li>
  <li><strong>Comparison Script</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python compare_experiments.py <span class="se">\</span>
    <span class="nt">--federated-dir</span> outputs/fl_run_2025-10-11 <span class="se">\</span>
    <span class="nt">--centralized-dir</span> outputs/centralized_run <span class="se">\</span>
    <span class="nt">--metrics</span> policy_loss,success_rate <span class="se">\</span>
    <span class="nt">--seed</span> 42
</code></pre></div>    </div>
  </li>
  <li><strong>Statistical Testing</strong>: 95% CI on metrics; expect federated within 10-20% of centralized.</li>
</ul>

<h2 id="technical-decisions">Technical Decisions</h2>

<h3 id="framework-selection">Framework Selection</h3>
<ul>
  <li><strong>Flower</strong>: Simplicity, PyTorch integration, scalability (v1.21.0 with Ray 2.31.0).</li>
  <li><strong>SmolVLA</strong>: Efficient VLA (layer skipping, visual token reduction) for robotics.</li>
  <li><strong>SO-100 Datasets</strong>: Standardized format with 30 FPS, multi-view cameras.</li>
</ul>

<h3 id="key-patterns">Key Patterns</h3>
<ul>
  <li><strong>Modular Design</strong>: Separate client/server apps, utils for shared logic.</li>
  <li><strong>Code Refactoring</strong>: Modularized aggregate_fit() in server_app.py for better maintainability (v0.3.6).</li>
  <li><strong>HF Push Logic</strong>: Conditional push to Hugging Face Hub for full runs (≥20 rounds) to avoid incomplete checkpoints.</li>
  <li><strong>Config System</strong>: <code class="language-plaintext highlighter-rouge">pyproject.toml</code> for FL params, <code class="language-plaintext highlighter-rouge">.env</code> for secrets, YAML for datasets.</li>
  <li><strong>Error Handling</strong>: Fail-fast with clear messages; no mocks in production.</li>
  <li><strong>Logging</strong>: Unified via Loguru (<code class="language-plaintext highlighter-rouge">simulation.log</code>); client-prefixed metrics.</li>
</ul>

<h3 id="scalability--performance">Scalability &amp; Performance</h3>
<ul>
  <li><strong>Horizontal Scaling</strong>: 10+ clients in simulation.</li>
  <li><strong>GPU Optimization</strong>: AMP support, memory streaming.</li>
  <li><strong>FedProx Benefits</strong>: Stabilizes convergence on heterogeneous data.</li>
  <li>Benchmarks: 78.3% SO-100 success rate; 30% faster async inference.</li>
</ul>

<h2 id="planned-enhancements">Planned Enhancements</h2>

<ul>
  <li>Multi-task learning across SO-100 variants.</li>
  <li>Advanced strategies (SCAFFOLD).</li>
  <li>Hyperparam auto-tuning.</li>
  <li>ZK proofs for verifiable contributions.</li>
</ul>

<h2 id="current-config-v026">Current Config (v0.2.6)</h2>

<ul>
  <li><strong>Clients</strong>: 4 (pyproject.toml <code class="language-plaintext highlighter-rouge">[tool.zk0.datasets]</code>):
    <ul>
      <li>Client 0: <code class="language-plaintext highlighter-rouge">lerobot/svla_so100_pickplace</code> (50 episodes) ✅ CLEAN</li>
      <li>Client 1: <code class="language-plaintext highlighter-rouge">lerobot/svla_so100_stacking</code> (56 episodes) ✅ HOTFIX</li>
      <li>Client 2: <code class="language-plaintext highlighter-rouge">lerobot/svla_so100_sorting</code> (52 episodes) ✅ HOTFIX</li>
      <li>Client 3: <code class="language-plaintext highlighter-rouge">lerobot/svla_so101_pickplace</code> (50 episodes) ✅ CLEAN</li>
    </ul>
  </li>
  <li><strong>Rounds</strong>: 30-50 (local-epochs=20-50, batch=64, warm restarts T_0=15).</li>
  <li><strong>Model</strong>: <code class="language-plaintext highlighter-rouge">lerobot/smolvla_base</code>.</li>
  <li><strong>Scheduling</strong>: Cosine warm restarts (T_0=15, T_mult=1.2), adaptive LR (1.15x boost for hard clients), dynamic mu (1.05x on high std).</li>
  <li><strong>Eval</strong>: Policy loss on unseen SO-101; videos for qualitative.</li>
  <li><strong>Status</strong>: v0.2.6 – Advanced LR/MU scheduling for &lt;0.15 server loss target.</li>
</ul>

<p>For implementation details, see source files like <a href="src/task.py"><code class="language-plaintext highlighter-rouge">src/task.py</code></a> for training/eval logic.</p>

<h2 id="references">References</h2>
<ul>
  <li>Flower Docs: <a href="https://flower.ai/docs/examples/quickstart-lerobot.html">Quickstart-LeRobot</a>.</li>
  <li>LeRobot: <a href="https://huggingface.co/docs/lerobot/smolvla">SmolVLA</a>.</li>
  <li>LeRobot: <a href="https://huggingface.co/docs/lerobot/evaluation">Evaluation Docs</a>.</li>
  <li>Flower: <a href="https://flower.ai/docs/framework/how-to-run-simulations.html">Simulation Guide</a>.</li>
  <li>Tools: imageio for videos; cv2 for analysis.</li>
  <li><a href="DEVELOPMENT.md">DEVELOPMENT.md</a> for development guidelines and testing.</li>
  <li><a href="INSTALLATION.md">INSTALLATION.md</a> for environment setup and execution instructions.</li>
</ul>
