diff --git a/pyproject.toml b/pyproject.toml
index 25d81f1..4ef1d77 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -47,12 +47,12 @@ serverapp = "src.server_app:app"
 clientapp = "src.client_app:app"
 
 [tool.flwr.app.config]
-num-server-rounds = 10 # train rounds
-local-epochs = 20 # steps/epochs per round
+num-server-rounds = 2 # train rounds
+local-epochs = 2 # steps/epochs per round
 model-name = "lerobot/smolvla_base"
 fraction-fit = 1 # set to 1 for all clients to train each round
 fraction-evaluate = 1 # Evaluate all clients
-batch_size = 64 # Batch size for training and evaluation (match standalone train script)
+batch_size = 2 # Batch size for training and evaluation (match standalone train script)
 server-device = "gpu" # could be also "cpu"
 use-wandb = true # requires WANDB_API_KEY env var set
 eval-frequency = 1 # Evaluate every eval-frequency rounds
diff --git a/src/client_app.py b/src/client_app.py
index 5830275..45fc30b 100644
--- a/src/client_app.py
+++ b/src/client_app.py
@@ -17,7 +17,7 @@ from flwr.common import Context
 
 # Flower client
 class SmolVLAClient(NumPyClient):
-    def __init__(self, partition_id, local_epochs, trainloader, nn_device=None) -> None:
+    def __init__(self, partition_id, local_epochs, trainloader, nn_device=None, use_wandb=False) -> None:
         self.partition_id = partition_id
         self.trainloader = trainloader
         self.local_epochs = local_epochs
@@ -31,12 +31,28 @@ class SmolVLAClient(NumPyClient):
         self.trainer = SmolVLATrainer(
             client_id=partition_id,
             device=nn_device,
-            use_wandb=False,  # Will be set per round from config
+            use_wandb=use_wandb,
             dataset_meta=dataset_meta,
             local_epochs=local_epochs,
             batch_size=64  # Default, will be overridden per round if needed
         )
 
+        # Initialize WandB if enabled
+        if use_wandb:
+            from src.wandb_utils import init_wandb
+            dataset_name = dataset_meta.repo_id if hasattr(dataset_meta, 'repo_id') else "unknown"
+            self.wandb_run = init_wandb(
+                project="zk0",
+                name=f"client_{partition_id}_{dataset_name}",
+                config={
+                    "client_id": partition_id,
+                    "dataset": dataset_name,
+                    "local_epochs": local_epochs,
+                    "batch_size": 64,
+                },
+                notes=f"Federated Learning Client {partition_id} - Dataset: {dataset_name}"
+            )
+
         # Load model using trainer
         self.net = self.trainer.get_model()
 
@@ -110,7 +126,7 @@ class SmolVLAClient(NumPyClient):
             fedprox_mu=proximal_mu,
             initial_lr=initial_lr
         )
-        self.trainer.use_wandb = use_wandb
+        # Note: use_wandb is already set during client initialization
 
         # Update batch_size in trainer if different from default
         self.trainer.batch_size = batch_size
@@ -305,7 +321,8 @@ def client_fn(context: Context) -> Client:
         logging.warning(f"⚠️ Client {partition_id}: No log_file_path provided in config")
 
     batch_size = context.run_config.get("batch_size", 64)
-    logging.info(f"✅ Client {partition_id}: Batch size set to {batch_size}")
+    use_wandb = context.run_config.get("use-wandb", False)
+    logging.info(f"✅ Client {partition_id}: Batch size set to {batch_size}, use_wandb={use_wandb}")
 
     # Load dataset first to get metadata for model creation
     logging.info(f"📊 Client {partition_id}: Loading dataset (partition_id={partition_id}, num_partitions={num_partitions})")
@@ -327,6 +344,7 @@ def client_fn(context: Context) -> Client:
             local_epochs=local_epochs,
             trainloader=trainloader,
             nn_device=nn_device,
+            use_wandb=use_wandb,
         )
         logging.info(f"✅ Client {partition_id}: SmolVLAClient created successfully")
         logging.info(f"🚀 Client {partition_id}: Converting to Flower client")
diff --git a/src/server_app.py b/src/server_app.py
index 3f87420..b356c62 100644
--- a/src/server_app.py
+++ b/src/server_app.py
@@ -631,28 +631,17 @@ def server_fn(context: Context) -> ServerAppComponents:
     # Add app-specific configs to context.run_config for strategy access
     context.run_config["checkpoint_interval"] = app_config.get("checkpoint_interval", 2)
 
-    # Initialize wandb if enabled and API key is available
+    # Initialize WandB if enabled
+    from src.wandb_utils import init_wandb
     wandb_run = None
     if app_config.get("use-wandb", False):
-        try:
-            import os
-            import wandb
-
-            wandb_api_key = os.environ.get("WANDB_API_KEY")
-            if wandb_api_key:
-                wandb_run = wandb.init(
-                    project="zk0",  # + Align with client project for unified dashboard
-                    name=f"fl-run-{folder_name}",
-                    config=dict(app_config),
-                    dir=str(save_path)
-                )
-                logger.info(f"Wandb initialized: {wandb_run.name} in project {wandb_run.project}")
-            else:
-                logger.warning("WANDB_API_KEY not found in environment variables. Wandb logging disabled.")
-        except ImportError:
-            logger.warning("wandb not available. Install with: pip install wandb")
-        except Exception as e:
-            logger.error(f"Failed to initialize wandb: {e}")
+        wandb_run = init_wandb(
+            project="zk0",
+            name=f"fl-run-{folder_name}",
+            config=dict(app_config),
+            dir=str(save_path),
+            notes=f"Federated Learning Server - {num_rounds} rounds"
+        )
 
     # Store wandb run in context for access by visualization functions
     context.run_config["wandb_run"] = wandb_run
diff --git a/src/task.py b/src/task.py
index f8328e5..b6b1740 100644
--- a/src/task.py
+++ b/src/task.py
@@ -145,13 +145,18 @@ def set_params(model, parameters) -> None:
     log_param_status(model, "pre-local training round: parameters sent from server to client")
 
 
-def setup_training_components(policy, trainloader, epochs, batch_size, device, initial_lr):
+def setup_training_components(policy, trainloader, epochs, batch_size, device, initial_lr, use_wandb=False, partition_id=None):
     """Setup training components: optimizer, scheduler, metrics, and configuration."""
     from lerobot.optim.factory import make_optimizer_and_scheduler
     from lerobot.configs.train import TrainPipelineConfig
+    from lerobot.configs.default import WandBConfig  # + Import WandBConfig for logging enablement
     from lerobot.utils.logging_utils import AverageMeter, MetricsTracker
     from torch.amp import GradScaler
 
+    # Create client-specific WandB configuration to prevent metric overlap
+    client_id = partition_id if partition_id is not None else "unknown"
+    dataset_name = trainloader.dataset.meta.repo_id if hasattr(trainloader.dataset, 'meta') else "unknown"
+
     # Create minimal config for lerobot factories (like standalone script)
     from lerobot.configs.default import DatasetConfig
     cfg = TrainPipelineConfig(
@@ -164,6 +169,13 @@ def setup_training_components(policy, trainloader, epochs, batch_size, device, i
         num_workers=0,
         log_freq=250,
         steps=epochs,
+        wandb=WandBConfig(  # + Enable WandB logging with zk0 project
+            project="zk0",
+            enable=use_wandb,  # Use parameter passed from Flower framework
+            mode="online",  # Use "offline" if no internet; defaults to online
+            run_id=f"client_{client_id}_{dataset_name}",  # Unique run ID per client
+            notes=f"Federated Learning Client {client_id} - Dataset: {dataset_name}",  # Client description
+        ),
     )
 
     # Use lerobot's optimizer factory
@@ -361,7 +373,7 @@ def run_training_loop(policy, trainloader, epochs, device, cfg, optimizer, lr_sc
     return step
 
 
-def train(net=None, trainloader=None, epochs=None, batch_size=64, device=None, global_params=None, fedprox_mu=0.01, initial_lr=None) -> dict[str, float]:
+def train(net=None, trainloader=None, epochs=None, batch_size=64, device=None, global_params=None, fedprox_mu=0.01, initial_lr=None, use_wandb=False, partition_id=None) -> dict[str, float]:
     """Train SmolVLA model using lerobot's training loop (reusing the provided model instance)."""
     import logging
 
@@ -376,7 +388,7 @@ def train(net=None, trainloader=None, epochs=None, batch_size=64, device=None, g
 
     # Setup training components
     cfg, optimizer, lr_scheduler, grad_scaler, train_metrics, train_tracker = setup_training_components(
-        policy, trainloader, epochs, batch_size, device, initial_lr
+        policy, trainloader, epochs, batch_size, device, initial_lr, use_wandb, partition_id
     )
 
     # Run training loop
@@ -698,9 +710,13 @@ class SmolVLATrainer:
         """Setup training components: optimizer, scheduler, metrics, and configuration."""
         from lerobot.optim.factory import make_optimizer_and_scheduler
         from lerobot.configs.train import TrainPipelineConfig
+        from lerobot.configs.default import WandBConfig  # + Import WandBConfig for logging enablement
         from lerobot.utils.logging_utils import AverageMeter, MetricsTracker
         from torch.amp import GradScaler
 
+        # Create client-specific WandB configuration to prevent metric overlap
+        dataset_name = self.trainloader.dataset.meta.repo_id if hasattr(self.trainloader.dataset, 'meta') else "unknown"
+
         # Create minimal config for lerobot factories (like standalone script)
         from lerobot.configs.default import DatasetConfig
         cfg = TrainPipelineConfig(
@@ -713,6 +729,13 @@ class SmolVLATrainer:
             num_workers=0,
             log_freq=250,
             steps=epochs,
+            wandb=WandBConfig(  # + Enable WandB logging with zk0 project
+                project="zk0",
+                enable=self.use_wandb,  # Use parameter passed from Flower framework
+                mode="online",  # Use "offline" if no internet; defaults to online
+                run_id=f"client_{self.client_id}_{dataset_name}",  # Unique run ID per client
+                notes=f"Federated Learning Client {self.client_id} - Dataset: {dataset_name}",  # Client description
+            ),
         )
 
         # Use lerobot's optimizer factory
@@ -927,27 +950,29 @@ class SmolVLATrainer:
         return final_metrics
 
     def _log_training_metrics(self, step: int, metrics: Dict[str, Any]) -> None:
-        """Log training metrics to WandB with low overhead."""
+        """Log comprehensive training metrics to WandB with clear loss breakdowns."""
         if not self.use_wandb:
             return
 
-        try:
-            import wandb
-            # Log every 20 steps to minimize overhead
-            if step % 20 == 0:
-                wandb.log({
-                    "client_id": self.client_id,
-                    "round": self.round_num,
-                    "step": step,
-                    "train_loss": self.train_metrics['loss'].avg,
-                    "learning_rate": self.train_metrics['lr'].avg,
-                    "grad_norm": self.train_metrics['grad_norm'].avg,
-                    "fedprox_loss": metrics.get('fedprox_loss', 0.0),
-                })
-        except ImportError:
-            logger.warning("WandB not available, skipping logging")
-        except Exception as e:
-            logger.warning(f"WandB logging failed: {e}")
+        # Log every 10 steps to capture frequent metrics for detailed analysis
+        if step % 10 == 0:
+            from src.wandb_utils import log_wandb_metrics
+
+            # Extract loss components for clear dashboard visualization
+            model_forward_loss = metrics.get('model_forward_loss', 0.0)  # Original LeRobot forward loss
+            fedprox_regularization_loss = metrics.get('fedprox_regularization_loss', 0.0)  # FedProx proximal term
+            total_training_loss = self.train_metrics['loss'].avg  # Final loss used for backprop (model_forward + fedprox)
+
+            log_wandb_metrics({
+                "federated_client_id": self.client_id,
+                "federated_round_number": self.round_num,
+                "training_step": step,
+                "model_forward_loss": model_forward_loss,  # Loss from SmolVLA forward pass (LeRobot)
+                "fedprox_regularization_loss": fedprox_regularization_loss,  # FedProx proximal regularization term
+                "total_training_loss": total_training_loss,  # Combined loss for model update (forward + regularization)
+                "learning_rate": self.train_metrics['lr'].avg,
+                "gradient_norm": self.train_metrics['grad_norm'].avg,
+            })
 
     def _log_system_metrics(self) -> None:
         """Log system metrics (GPU memory, etc.) to WandB."""
