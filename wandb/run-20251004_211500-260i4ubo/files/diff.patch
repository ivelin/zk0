diff --git a/pyproject.toml b/pyproject.toml
index 25d81f1..5e89014 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -47,7 +47,7 @@ serverapp = "src.server_app:app"
 clientapp = "src.client_app:app"
 
 [tool.flwr.app.config]
-num-server-rounds = 10 # train rounds
+num-server-rounds = 20 # train rounds
 local-epochs = 20 # steps/epochs per round
 model-name = "lerobot/smolvla_base"
 fraction-fit = 1 # set to 1 for all clients to train each round
diff --git a/src/client_app.py b/src/client_app.py
index 5830275..f91c3b3 100644
--- a/src/client_app.py
+++ b/src/client_app.py
@@ -17,7 +17,7 @@ from flwr.common import Context
 
 # Flower client
 class SmolVLAClient(NumPyClient):
-    def __init__(self, partition_id, local_epochs, trainloader, nn_device=None) -> None:
+    def __init__(self, partition_id, local_epochs, trainloader, nn_device=None, use_wandb=False, wandb_group=None, batch_size=64) -> None:
         self.partition_id = partition_id
         self.trainloader = trainloader
         self.local_epochs = local_epochs
@@ -31,12 +31,44 @@ class SmolVLAClient(NumPyClient):
         self.trainer = SmolVLATrainer(
             client_id=partition_id,
             device=nn_device,
-            use_wandb=False,  # Will be set per round from config
+            use_wandb=use_wandb,
             dataset_meta=dataset_meta,
             local_epochs=local_epochs,
-            batch_size=64  # Default, will be overridden per round if needed
+            batch_size=batch_size  # Use the batch_size passed from server config
         )
 
+        # Initialize WandB if enabled
+        if use_wandb:
+            from src.wandb_utils import init_wandb
+            dataset_name = dataset_meta.repo_id if hasattr(dataset_meta, 'repo_id') else "unknown"
+            if wandb_group:
+                # Group under server's run
+                self.wandb_run = init_wandb(
+                    project="zk0",
+                    name=f"client_{partition_id}",
+                    group=wandb_group,
+                    config={
+                        "client_id": partition_id,
+                        "dataset": dataset_name,
+                        "local_epochs": local_epochs,
+                        "batch_size": batch_size,
+                    },
+                    notes=f"Federated Learning Client {partition_id} - Dataset: {dataset_name}"
+                )
+            else:
+                # Fallback: create separate run (legacy behavior)
+                self.wandb_run = init_wandb(
+                    project="zk0",
+                    name=f"client_{partition_id}_{dataset_name}",
+                    config={
+                        "client_id": partition_id,
+                        "dataset": dataset_name,
+                        "local_epochs": local_epochs,
+                        "batch_size": batch_size,
+                    },
+                    notes=f"Federated Learning Client {partition_id} - Dataset: {dataset_name}"
+                )
+
         # Load model using trainer
         self.net = self.trainer.get_model()
 
@@ -110,7 +142,7 @@ class SmolVLAClient(NumPyClient):
             fedprox_mu=proximal_mu,
             initial_lr=initial_lr
         )
-        self.trainer.use_wandb = use_wandb
+        # Note: use_wandb is already set during client initialization
 
         # Update batch_size in trainer if different from default
         self.trainer.batch_size = batch_size
@@ -305,7 +337,9 @@ def client_fn(context: Context) -> Client:
         logging.warning(f"âš ï¸ Client {partition_id}: No log_file_path provided in config")
 
     batch_size = context.run_config.get("batch_size", 64)
-    logging.info(f"âœ… Client {partition_id}: Batch size set to {batch_size}")
+    use_wandb = context.run_config.get("use-wandb", False)
+    wandb_group = context.run_config.get("wandb_group")
+    logging.info(f"âœ… Client {partition_id}: Batch size set to {batch_size}, use_wandb={use_wandb}, wandb_group={wandb_group}")
 
     # Load dataset first to get metadata for model creation
     logging.info(f"ðŸ“Š Client {partition_id}: Loading dataset (partition_id={partition_id}, num_partitions={num_partitions})")
@@ -327,6 +361,9 @@ def client_fn(context: Context) -> Client:
             local_epochs=local_epochs,
             trainloader=trainloader,
             nn_device=nn_device,
+            use_wandb=use_wandb,
+            wandb_group=wandb_group,
+            batch_size=batch_size,
         )
         logging.info(f"âœ… Client {partition_id}: SmolVLAClient created successfully")
         logging.info(f"ðŸš€ Client {partition_id}: Converting to Flower client")
diff --git a/src/server_app.py b/src/server_app.py
index 3f87420..bc0e2f1 100644
--- a/src/server_app.py
+++ b/src/server_app.py
@@ -246,6 +246,17 @@ class AggregateEvaluationStrategy(FedProx):
             updated_fit_config["use_wandb"] = use_wandb
             logger.debug(f"Server: Added use_wandb={use_wandb} to client {client_proxy.cid} config")
 
+            # Add batch_size from pyproject.toml to override client defaults
+            batch_size = app_config.get("batch_size", 64)
+            updated_fit_config["batch_size"] = batch_size
+            logger.debug(f"Server: Added batch_size={batch_size} to client {client_proxy.cid} config")
+
+            # Add WandB group for unified logging
+            wandb_group = self.context.run_config.get("wandb_group")
+            if wandb_group:
+                updated_fit_config["wandb_group"] = wandb_group
+                logger.debug(f"Server: Added wandb_group={wandb_group} to client {client_proxy.cid} config")
+
             # ðŸ›¡ï¸ VALIDATE: Server outgoing parameters (for training)
             from src.utils import validate_and_log_parameters
             from flwr.common import parameters_to_ndarrays
@@ -299,6 +310,17 @@ class AggregateEvaluationStrategy(FedProx):
             updated_evaluate_config["use_wandb"] = use_wandb
             logger.debug(f"Server: Added use_wandb={use_wandb} to client {client_proxy.cid} eval config")
 
+            # Add batch_size from pyproject.toml to override client defaults (for eval)
+            batch_size = app_config.get("batch_size", 64)
+            updated_evaluate_config["batch_size"] = batch_size
+            logger.debug(f"Server: Added batch_size={batch_size} to client {client_proxy.cid} eval config")
+
+            # Add WandB group for unified logging (for eval)
+            wandb_group = self.context.run_config.get("wandb_group")
+            if wandb_group:
+                updated_evaluate_config["wandb_group"] = wandb_group
+                logger.debug(f"Server: Added wandb_group={wandb_group} to client {client_proxy.cid} eval config")
+
             # ðŸ›¡ï¸ VALIDATE: Server outgoing parameters (for evaluation)
             from src.utils import validate_and_log_parameters
             eval_param_hash = validate_and_log_parameters(
@@ -425,6 +447,27 @@ class AggregateEvaluationStrategy(FedProx):
                 # Also plot federated metrics if we have them
                 if hasattr(self, 'federated_metrics_history') and self.federated_metrics_history:
                     visualizer.plot_federated_metrics(self.federated_metrics_history, self.server_dir, wandb_run=self.wandb_run)
+
+                # Log final aggregated metrics to WandB
+                if self.wandb_run and aggregated_metrics:
+                    from src.wandb_utils import log_wandb_metrics
+                    final_metrics = {
+                        "final_round": server_round,
+                        "final_server_avg_mse": aggregated_metrics.get("avg_action_mse", 0.0),
+                        "num_clients": len(results),
+                        "fraction_fit": self.fraction_fit,
+                        "fraction_evaluate": self.fraction_evaluate,
+                        "checkpoint_interval": self.context.run_config.get("checkpoint_interval", 2),
+                        "eval_frequency": self.context.run_config.get("eval-frequency", 5),
+                        "eval_mode": self.context.run_config.get("eval_mode", "quick"),
+                        "proximal_mu": self.proximal_mu,
+                        "num_server_rounds": self.num_rounds,
+                        "model_name": self.context.run_config.get("model-name", "lerobot/smolvla_base"),
+                        "hf_repo_id": self.context.run_config.get("hf_repo_id"),
+                    }
+                    log_wandb_metrics(final_metrics)
+                    logger.info(f"Logged final aggregated metrics to WandB: {final_metrics}")
+
                 logger.info("Eval MSE chart generated for final round")
             except Exception as e:
                 logger.error(f"Failed to generate eval MSE chart: {e}")
@@ -631,28 +674,19 @@ def server_fn(context: Context) -> ServerAppComponents:
     # Add app-specific configs to context.run_config for strategy access
     context.run_config["checkpoint_interval"] = app_config.get("checkpoint_interval", 2)
 
-    # Initialize wandb if enabled and API key is available
+    # Initialize WandB if enabled
+    from src.wandb_utils import init_wandb
     wandb_run = None
+    wandb_group = f"fl-run-{folder_name}"  # Group name for all clients and server
     if app_config.get("use-wandb", False):
-        try:
-            import os
-            import wandb
-
-            wandb_api_key = os.environ.get("WANDB_API_KEY")
-            if wandb_api_key:
-                wandb_run = wandb.init(
-                    project="zk0",  # + Align with client project for unified dashboard
-                    name=f"fl-run-{folder_name}",
-                    config=dict(app_config),
-                    dir=str(save_path)
-                )
-                logger.info(f"Wandb initialized: {wandb_run.name} in project {wandb_run.project}")
-            else:
-                logger.warning("WANDB_API_KEY not found in environment variables. Wandb logging disabled.")
-        except ImportError:
-            logger.warning("wandb not available. Install with: pip install wandb")
-        except Exception as e:
-            logger.error(f"Failed to initialize wandb: {e}")
+        wandb_run = init_wandb(
+            project="zk0",
+            name=f"server-{folder_name}",
+            group=wandb_group,
+            config=dict(app_config),
+            dir=str(save_path),
+            notes=f"Federated Learning Server - {num_rounds} rounds"
+        )
 
     # Store wandb run in context for access by visualization functions
     context.run_config["wandb_run"] = wandb_run
@@ -660,6 +694,7 @@ def server_fn(context: Context) -> ServerAppComponents:
     # Add save_path and log_file_path to run config for clients (for client log paths)
     context.run_config["log_file_path"] = str(simulation_log_path)
     context.run_config["save_path"] = str(save_path)
+    context.run_config["wandb_group"] = wandb_group  # Pass WandB group to clients for unified logging
 
     # Save configuration snapshot
     import json
diff --git a/src/task.py b/src/task.py
index f8328e5..fb00f7a 100644
--- a/src/task.py
+++ b/src/task.py
@@ -145,13 +145,18 @@ def set_params(model, parameters) -> None:
     log_param_status(model, "pre-local training round: parameters sent from server to client")
 
 
-def setup_training_components(policy, trainloader, epochs, batch_size, device, initial_lr):
+def setup_training_components(policy, trainloader, epochs, batch_size, device, initial_lr, use_wandb=False, partition_id=None):
     """Setup training components: optimizer, scheduler, metrics, and configuration."""
     from lerobot.optim.factory import make_optimizer_and_scheduler
     from lerobot.configs.train import TrainPipelineConfig
+    from lerobot.configs.default import WandBConfig  # + Import WandBConfig for logging enablement
     from lerobot.utils.logging_utils import AverageMeter, MetricsTracker
     from torch.amp import GradScaler
 
+    # Create client-specific WandB configuration to prevent metric overlap
+    client_id = partition_id if partition_id is not None else "unknown"
+    dataset_name = trainloader.dataset.meta.repo_id if hasattr(trainloader.dataset, 'meta') else "unknown"
+
     # Create minimal config for lerobot factories (like standalone script)
     from lerobot.configs.default import DatasetConfig
     cfg = TrainPipelineConfig(
@@ -164,6 +169,13 @@ def setup_training_components(policy, trainloader, epochs, batch_size, device, i
         num_workers=0,
         log_freq=250,
         steps=epochs,
+        wandb=WandBConfig(  # + Enable WandB logging with zk0 project
+            project="zk0",
+            enable=use_wandb,  # Use parameter passed from Flower framework
+            mode="online",  # Use "offline" if no internet; defaults to online
+            run_id=f"client_{client_id}_{dataset_name}",  # Unique run ID per client
+            notes=f"Federated Learning Client {client_id} - Dataset: {dataset_name}",  # Client description
+        ),
     )
 
     # Use lerobot's optimizer factory
@@ -296,7 +308,7 @@ def run_training_step(step, policy, batch, device, train_tracker, optimizer, gra
         logger.debug(f"Step {step}: VRAM after empty_cache - Allocated: {cleared_allocated:.2f} GB, Reserved: {cleared_reserved:.2f} GB, Free: {cleared_free_gb:.2f} GB / {total_memory_gb:.2f} GB")
 
     logger.debug(f"Step {step}: Training step completed successfully. Total loss: {total_loss.item():.6f}")
-    return train_tracker, output_dict
+    return train_tracker, output_dict, main_loss.item(), proximal_loss.item()
 
 
 def run_training_loop(policy, trainloader, epochs, device, cfg, optimizer, lr_scheduler, grad_scaler, train_metrics, train_tracker, global_params, fedprox_mu):
@@ -337,7 +349,7 @@ def run_training_loop(policy, trainloader, epochs, device, cfg, optimizer, lr_sc
         logger.debug(f"Step {step}: Batch fetched successfully. Keys: {list(batch.keys())}, Sample shapes: {{k: v.shape if hasattr(v, 'shape') else type(v) for k,v in batch.items()}}")
 
         # Run training step
-        train_tracker, output_dict = run_training_step(
+        train_tracker, output_dict, main_loss_val, proximal_loss_val = run_training_step(
             step, policy, batch, device, train_tracker, optimizer, grad_scaler,
             lr_scheduler, cfg, global_params, fedprox_mu
         )
@@ -361,7 +373,7 @@ def run_training_loop(policy, trainloader, epochs, device, cfg, optimizer, lr_sc
     return step
 
 
-def train(net=None, trainloader=None, epochs=None, batch_size=64, device=None, global_params=None, fedprox_mu=0.01, initial_lr=None) -> dict[str, float]:
+def train(net=None, trainloader=None, epochs=None, batch_size=64, device=None, global_params=None, fedprox_mu=0.01, initial_lr=None, use_wandb=False, partition_id=None) -> dict[str, float]:
     """Train SmolVLA model using lerobot's training loop (reusing the provided model instance)."""
     import logging
 
@@ -376,7 +388,7 @@ def train(net=None, trainloader=None, epochs=None, batch_size=64, device=None, g
 
     # Setup training components
     cfg, optimizer, lr_scheduler, grad_scaler, train_metrics, train_tracker = setup_training_components(
-        policy, trainloader, epochs, batch_size, device, initial_lr
+        policy, trainloader, epochs, batch_size, device, initial_lr, use_wandb, partition_id
     )
 
     # Run training loop
@@ -698,9 +710,13 @@ class SmolVLATrainer:
         """Setup training components: optimizer, scheduler, metrics, and configuration."""
         from lerobot.optim.factory import make_optimizer_and_scheduler
         from lerobot.configs.train import TrainPipelineConfig
+        from lerobot.configs.default import WandBConfig  # + Import WandBConfig for logging enablement
         from lerobot.utils.logging_utils import AverageMeter, MetricsTracker
         from torch.amp import GradScaler
 
+        # Create client-specific WandB configuration to prevent metric overlap
+        dataset_name = self.trainloader.dataset.meta.repo_id if hasattr(self.trainloader.dataset, 'meta') else "unknown"
+
         # Create minimal config for lerobot factories (like standalone script)
         from lerobot.configs.default import DatasetConfig
         cfg = TrainPipelineConfig(
@@ -713,6 +729,13 @@ class SmolVLATrainer:
             num_workers=0,
             log_freq=250,
             steps=epochs,
+            wandb=WandBConfig(  # + Enable WandB logging with zk0 project
+                project="zk0",
+                enable=self.use_wandb,  # Use parameter passed from Flower framework
+                mode="online",  # Use "offline" if no internet; defaults to online
+                run_id=f"client_{self.client_id}_{dataset_name}",  # Unique run ID per client
+                notes=f"Federated Learning Client {self.client_id} - Dataset: {dataset_name}",  # Client description
+            ),
         )
 
         # Use lerobot's optimizer factory
@@ -824,7 +847,7 @@ class SmolVLATrainer:
             logger.debug(f"Step {step}: VRAM after empty_cache - Allocated: {cleared_allocated:.2f} GB, Reserved: {cleared_reserved:.2f} GB, Free: {cleared_free_gb:.2f} GB / {total_memory_gb:.2f} GB")
 
         logger.debug(f"Step {step}: update_policy completed successfully. Loss from output: {output_dict.get('loss', 'N/A')}")
-        return self.train_tracker, output_dict
+        return self.train_tracker, output_dict, output_dict.get('loss', 0.0), 0.0  # Return loss and fedprox_loss (placeholder)
 
     def run_training_loop(self, epochs: int) -> int:
         """Run the main training loop."""
@@ -864,7 +887,7 @@ class SmolVLATrainer:
             logger.debug(f"Step {step}: Batch fetched successfully. Keys: {list(batch.keys())}, Sample shapes: {{k: v.shape if hasattr(v, 'shape') else type(v) for k,v in batch.items()}}")
 
             # Run training step
-            self.train_tracker, output_dict = self.run_training_step(step, batch)
+            self.train_tracker, output_dict, main_loss_val, proximal_loss_val = self.run_training_step(step, batch)
 
             step += 1
             self.train_tracker.step()
@@ -881,8 +904,12 @@ class SmolVLATrainer:
                 logger.info(f"Step {step}: loss={self.train_metrics['loss'].avg:.4f}, grad_norm={self.train_metrics['grad_norm'].avg:.4f}, lr={self.train_metrics['lr'].avg:.4f}, update_s={self.train_metrics['update_s'].avg:.4f}")
                 self.train_tracker.reset_averages()
 
-            # Log to WandB with low overhead (every 20 steps)
-            self._log_training_metrics(step, {"fedprox_loss": 0.0})  # FedProx loss tracking could be added here
+            # Log to WandB with low overhead (every 10 steps in _log_training_metrics)
+            self._log_training_metrics(step, {
+                "model_forward_loss": main_loss_val,
+                "fedprox_regularization_loss": proximal_loss_val,
+                "skipped_batches": skipped_batches
+            })
 
         logger.info(f"Training completed after {step} steps (target: {epochs}), total time: {time.perf_counter() - loop_start_time:.2f}s, skipped_batches={skipped_batches}")
         return step
@@ -927,27 +954,31 @@ class SmolVLATrainer:
         return final_metrics
 
     def _log_training_metrics(self, step: int, metrics: Dict[str, Any]) -> None:
-        """Log training metrics to WandB with low overhead."""
+        """Log comprehensive training metrics to WandB with clear loss breakdowns."""
         if not self.use_wandb:
             return
 
-        try:
-            import wandb
-            # Log every 20 steps to minimize overhead
-            if step % 20 == 0:
-                wandb.log({
-                    "client_id": self.client_id,
-                    "round": self.round_num,
-                    "step": step,
-                    "train_loss": self.train_metrics['loss'].avg,
-                    "learning_rate": self.train_metrics['lr'].avg,
-                    "grad_norm": self.train_metrics['grad_norm'].avg,
-                    "fedprox_loss": metrics.get('fedprox_loss', 0.0),
-                })
-        except ImportError:
-            logger.warning("WandB not available, skipping logging")
-        except Exception as e:
-            logger.warning(f"WandB logging failed: {e}")
+        # Log every 10 steps to capture frequent metrics for detailed analysis
+        if step % 10 == 0:
+            from src.wandb_utils import log_wandb_metrics
+
+            # Extract loss components for clear dashboard visualization
+            model_forward_loss = metrics.get('model_forward_loss', 0.0)  # Original LeRobot forward loss
+            fedprox_regularization_loss = metrics.get('fedprox_regularization_loss', 0.0)  # FedProx proximal term
+            total_training_loss = self.train_metrics['loss'].avg  # Final loss used for backprop (model_forward + fedprox)
+
+            log_wandb_metrics({
+                "federated_client_id": self.client_id,
+                "federated_round_number": self.round_num,
+                "training_step": step,
+                "model_forward_loss": model_forward_loss,  # Loss from SmolVLA forward pass (LeRobot)
+                "fedprox_regularization_loss": fedprox_regularization_loss,  # FedProx proximal regularization term
+                "total_training_loss": total_training_loss,  # Combined loss for model update (forward + regularization)
+                "learning_rate": self.train_metrics['lr'].avg,
+                "gradient_norm": self.train_metrics['grad_norm'].avg,
+                "steps_completed": step,
+                "skipped_batches": metrics.get('skipped_batches', 0),
+            })
 
     def _log_system_metrics(self) -> None:
         """Log system metrics (GPU memory, etc.) to WandB."""
@@ -1022,7 +1053,7 @@ class SmolVLATrainer:
         total_batches_processed = 0
 
         # Set evaluation limit based on mode
-        max_batches_for_eval = 10 if eval_mode == "quick" else None
+        max_batches_for_eval = 3 if eval_mode == "quick" else None
 
         # Evaluate all batches in the dataloader
         for batch in eval_loader:
